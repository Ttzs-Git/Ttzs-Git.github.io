<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HDU 100:两个月挑战</title>
    <url>/2025/05/08/HDU-100-%E4%B8%A4%E4%B8%AA%E6%9C%88%E6%8C%91%E6%88%98/</url>
    <content><![CDATA[<h2 id="情况"><a href="#情况" class="headerlink" title="情况"></a>情况</h2><p>大一学了C++的基础知识和极其少的stl内容，没学数据结构与算法</p>
<p>刷HDU的2000-2099 检测一下C++的情况</p>
<p>每天**1.5h(3~4题)**的刷题,以及不限制限制时间的反思</p>
<p><strong>半个小时，不会就跳，结束后复盘</strong></p>
<p><strong>我的代码仓库:<a class="link"   href="https://github.com/Ttzs-Git/learning-logs/" >https://github.com/Ttzs-Git/learning-logs/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></strong></p>
<p>具体位置: &#x2F;learning-CPP-language&#x2F;cpp-algo</p>
<h2 id="可能常用网站"><a href="#可能常用网站" class="headerlink" title="可能常用网站"></a>可能常用网站</h2><p><a class="link"   href="https://acm.hdu.edu.cn/listproblem.php?vol=11" >HDU OJ<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><a class="link"   href="https://blog.csdn.net/m0_62434776/article/details/125602451" >题解1<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><a class="link"   href="https://blog.csdn.net/qq_38687678/article/details/83012480" >题解2<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h2 id="解题反思"><a href="#解题反思" class="headerlink" title="解题反思"></a>解题反思</h2><h3 id="HDU-2000"><a href="#HDU-2000" class="headerlink" title="HDU 2000"></a>HDU 2000</h3><p>我的代码中主要使用sort和vector，因此思路很简单。</p>
<ul>
<li>题解1:使用冒泡排序(问题: 感觉写的比较死,无法支持数量改变的数据)，不过正好练习一下<strong>冒泡排序</strong>的代码</li>
<li>两个题解都是用数组写的</li>
<li>题解2:EOF对HDU 2003挺重要的</li>
</ul>
<h3 id="HDU-2001"><a href="#HDU-2001" class="headerlink" title="HDU 2001"></a>HDU 2001</h3><p>我的代码主要使用vector和sqrt写的，</p>
<p>本题的关键点:<strong>题目中提示四个实数</strong></p>
<ul>
<li><p>题解1: 使用的cin&gt;&gt;a&gt;&gt;b&gt;&gt;c&gt;&gt;d—-这让我思考一个问题;<strong>是否全部的输入结束以后才能开始输出？</strong></p>
</li>
<li><p>题解2:使用scanf的返回值是输入值的个数，也可以用在HDU 2003</p>
</li>
<li><p>小数点后的位数使用printf的格式化控制比较简单</p>
</li>
</ul>
<h3 id="HDU-2002"><a href="#HDU-2002" class="headerlink" title="HDU 2002"></a>HDU 2002</h3><p>本题的关键: <strong>整数整除和float，double的测试</strong></p>
<p>注意:</p>
<ul>
<li>4&#x2F;3&#x3D;1;—给忘记了</li>
</ul>
<h3 id="HDU-2003"><a href="#HDU-2003" class="headerlink" title="HDU 2003"></a>HDU 2003</h3><ul>
<li>题解一: 提醒使用abs函数</li>
</ul>
<h3 id="HDU-2004"><a href="#HDU-2004" class="headerlink" title="HDU 2004"></a>HDU 2004</h3><ul>
<li>题解二: 字符数组挺不错的，可以代替冗长的switch</li>
</ul>
<h3 id="HDU-2005"><a href="#HDU-2005" class="headerlink" title="HDU 2005"></a>HDU 2005</h3><ul>
<li>题解一二: 冗长的switch可以使用数组代替</li>
</ul>
<h3 id="HDU-2006"><a href="#HDU-2006" class="headerlink" title="HDU 2006"></a>HDU 2006</h3><ul>
<li>题解一:使用num&amp;1判断奇数</li>
</ul>
<h3 id="HDU-2007"><a href="#HDU-2007" class="headerlink" title="HDU 2007"></a>HDU 2007</h3><p>由于没有评测机，所以只能使用样例。发现自己忽略了x和y的大小关系</p>
<ul>
<li>题解一:使用swap</li>
</ul>
<h3 id="HDU-2008"><a href="#HDU-2008" class="headerlink" title="HDU 2008"></a>HDU 2008</h3><ul>
<li>题解一: #include&lt;bits&#x2F;stdc++.h&gt;</li>
<li>题解二: 将t&#x3D;&#x3D;0的判断放到while中</li>
</ul>
<h3 id="HDU-2012"><a href="#HDU-2012" class="headerlink" title="HDU 2012"></a>HDU 2012</h3><ul>
<li>题解一: !(x&#x3D;=0&amp;&amp;y&#x3D;&#x3D;0)等价于x!&#x3D;0||y!&#x3D;0</li>
</ul>
<h3 id="HDU-2014"><a href="#HDU-2014" class="headerlink" title="HDU 2014"></a>HDU 2014</h3><p>我的思路简单: 先输入数据，然后排序，然后计算</p>
<ul>
<li>题解二: 找出最大值和最小值</li>
</ul>
<h3 id="HDU-2015"><a href="#HDU-2015" class="headerlink" title="HDU 2015"></a>HDU 2015</h3><p>推荐题解一的代码更加简洁明了。两份题解对我的起始，不要一味最求生成向量–计算–输出的思路。同时，生成的的代码也较复杂。复现题解一</p>
<h3 id="HDU-2018"><a href="#HDU-2018" class="headerlink" title="HDU 2018"></a>HDU 2018</h3><p>我的题解是使用递归写的，n&lt;&#x3D;4,输出原值，n&gt;5，递归</p>
<ul>
<li>题解1: 把n&lt;56的全部存到一个数组中，当n较大时，递归可能会太深</li>
<li>题解2: 不储存数据的迭代</li>
</ul>
<h3 id="HDU-2019"><a href="#HDU-2019" class="headerlink" title="HDU 2019"></a>HDU 2019</h3><p>0x3f3f3f3f-表示无穷大的数,凄凉被不超过int</p>
<ul>
<li>题解1: 使用m-无穷大,类似于标签—我改变了一下，在仓库中</li>
</ul>
<p>它的题解的输入和输出如下时，无法通过<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\HDU-100-两个月挑战\image-20250509232346551.png"
                      alt="image-20250509232346551"
                ></p>
<p>但是HDU oj中没有这个案例，是不是不需要考虑这种情况？</p>
<h3 id="HDU-2020"><a href="#HDU-2020" class="headerlink" title="HDU 2020"></a>HDU 2020</h3><ul>
<li>题解12:提醒sort(…,cmp)自定义部分</li>
</ul>
<h3 id="HDU-2021"><a href="#HDU-2021" class="headerlink" title="HDU 2021"></a>HDU 2021</h3><p>我的思路很朴素: 先换大的，再换小的</p>
<ul>
<li>题解2: 使用数组和循环避免了我的超长部分(使用**&#x2F;和***)</li>
</ul>
<h3 id="HDU-2023"><a href="#HDU-2023" class="headerlink" title="HDU 2023"></a>HDU 2023</h3><p>我的痛，在这儿卡了好久，感觉思路没有啥问题，但是g++编译器过不了。最后全部用vector写的，C++编译器过了</p>
<p>感悟:</p>
<ul>
<li>写标准的C++语法，不适用变长数组</li>
</ul>
<h3 id="HDU-2024"><a href="#HDU-2024" class="headerlink" title="HDU 2024"></a>HDU 2024</h3><p>题解一二都没考虑标识符不能是关键字。</p>
<h3 id="HDU-2025"><a href="#HDU-2025" class="headerlink" title="HDU 2025"></a>HDU 2025</h3><p>推荐len来做,推荐使用for的auto &amp;st:string</p>
<h3 id="HDU-2026"><a href="#HDU-2026" class="headerlink" title="HDU 2026"></a>HDU 2026</h3><ul>
<li>题解一: i !&#x3D; 0 &amp;&amp; s[i - 1] &#x3D;&#x3D; ‘ ‘–值得我学习</li>
</ul>
<h3 id="HDU-2027"><a href="#HDU-2027" class="headerlink" title="HDU 2027"></a>HDU 2027</h3><p>注意getchar()放在循环体内还循环体外的区别</p>
<h3 id="HDU-2028"><a href="#HDU-2028" class="headerlink" title="HDU 2028"></a>HDU 2028</h3><p>关键在于 <strong>long long</strong></p>
<ul>
<li>题解一: #define int long long</li>
</ul>
<h3 id="HDU-2030"><a href="#HDU-2030" class="headerlink" title="HDU 2030"></a>HDU 2030</h3><p>汉字编码的时候字节首位是1，即&lt;0;</p>
<p>我的电脑汉字占用3个字节，评测机是2个字节</p>
<h3 id="HDU-2031"><a href="#HDU-2031" class="headerlink" title="HDU 2031"></a>HDU 2031</h3><ul>
<li>题解一: 栈+递归</li>
</ul>
<h3 id="HDU-2032"><a href="#HDU-2032" class="headerlink" title="HDU 2032"></a>HDU 2032</h3><ul>
<li>题解一二: 暴力生成，然后按照指示输出</li>
</ul>
<h3 id="HDU-2033"><a href="#HDU-2033" class="headerlink" title="HDU 2033"></a>HDU 2033</h3><ul>
<li>题解1: 使用取余和整除写得太好了</li>
</ul>
<h3 id="HDU-2034"><a href="#HDU-2034" class="headerlink" title="HDU 2034"></a>HDU 2034</h3><ul>
<li>题解一: 没有使用set2节省内存；学习了set的用法</li>
<li>题解二: 开了一个新的数组，存放A-B的结果</li>
</ul>
<h3 id="HDU-2035"><a href="#HDU-2035" class="headerlink" title="HDU 2035"></a>HDU 2035</h3><ul>
<li>题解一: 使* 和 % 进行迭代，非常简洁的</li>
</ul>
<h3 id="HDU-2036"><a href="#HDU-2036" class="headerlink" title="HDU 2036"></a>HDU 2036</h3><p>我的思路是分解多边形+海伦公式，最后WA。提醒:海伦公式对于非凸四边形的计算有难度(如: 五角星)</p>
<ul>
<li>题解一: 多边形面积公式(分析:<a class="link"   href="https://blog.csdn.net/dengjing1200/article/details/102404012" >hdu 2036(多边形面积公式）-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>)</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\HDU-100-两个月挑战\image-20250514151941109.png"
                      alt="image-20250514151941109"
                ></p>
<h3 id="HDU-2037"><a href="#HDU-2037" class="headerlink" title="HDU 2037"></a>HDU 2037</h3><p>我的思路是使用开头的时间排序，可能造成一个有空隙，选择总长度较长的选择。建议: 使用结束时间排序</p>
<ul>
<li>题解一推荐阅读</li>
</ul>
<h3 id="HDU-2038"><a href="#HDU-2038" class="headerlink" title="HDU 2038"></a>HDU 2038</h3><p>没有HDU 2038</p>
<h3 id="HDU-2039"><a href="#HDU-2039" class="headerlink" title="HDU 2039"></a>HDU 2039</h3><p>注意: 这是<strong>正数</strong></p>
<h3 id="HDU-2040"><a href="#HDU-2040" class="headerlink" title="HDU 2040"></a>HDU 2040</h3><p><strong>真约数分布在a&#x2F;2的左侧或者分布在sqrt(a)的两侧</strong></p>
<h3 id="HDU-2041"><a href="#HDU-2041" class="headerlink" title="HDU 2041"></a>HDU 2041</h3><p>本题递归不会超时(我的代码没有超时)</p>
<p>题解一二都是打表完成</p>
<h3 id="HDU-2044"><a href="#HDU-2044" class="headerlink" title="HDU 2044"></a>HDU 2044</h3><p>本题尤其注意兔子数列的增速，大约40+的时候就超出了int，需要设置为long long</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\HDU-100-两个月挑战\image-20250515162059074.png"
                      alt="image-20250515162059074"
                ></p>
<h3 id="HDU-2045"><a href="#HDU-2045" class="headerlink" title="HDU 2045"></a>HDU 2045</h3><p>本题–需要递归的思考方式，但是递归会TLE，所以要写迭代的，同时，要开long long</p>
<h3 id="HDU-2046"><a href="#HDU-2046" class="headerlink" title="HDU 2046"></a>HDU 2046</h3><p>本题的迭代关系是fib的关系。</p>
<p>s(n)表示2*n的方案。铺骨牌的方式由两种竖着铺一张和横着铺两张。因此s(n)&#x3D;s(n-1)*1+s(n-2)*2;</p>
<h3 id="HDU-2047"><a href="#HDU-2047" class="headerlink" title="HDU 2047"></a>HDU 2047</h3><p>本题的迭代关系时f(n)&#x3D;f(n-1)*2+f(n-2)*2.通过将最后一位是非为O，进行分类计算</p>
<h3 id="HDU-2048"><a href="#HDU-2048" class="headerlink" title="HDU 2048"></a>HDU 2048</h3><p><strong>错位排列</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\HDU-100-两个月挑战\image-20250518184611571.png"
                      alt="image-20250518184611571"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\HDU-100-两个月挑战\image-20250518184740228.png"
                      alt="image-20250518184740228"
                ></p>
<h3 id="HDU-2049"><a href="#HDU-2049" class="headerlink" title="HDU 2049"></a>HDU 2049</h3><p>本体与上一题类似进加入一个组合数的运算</p>
<p>由于后续的答案较大，容易造成溢出，然后WA</p>
<h3 id="HDU-2050"><a href="#HDU-2050" class="headerlink" title="HDU 2050"></a>HDU 2050</h3><p>直线问题:</p>
<ul>
<li>内部区域的个数1+2+3+….+N-2</li>
<li>外部区域的个数2*N</li>
<li>总个数(N*N+N)&#x2F;2+1</li>
</ul>
<p>折线问题:(n是折线的个数)</p>
<ul>
<li>个数: f(n)&#x3D;f(n-1)+4(n-1)+1;</li>
<li>zn*n-n+1</li>
</ul>
<h3 id="HDU-2054"><a href="#HDU-2054" class="headerlink" title="HDU 2054"></a>HDU 2054</h3><p>大数字符串问题</p>
<p>不需要考虑前导0，不需要考虑有符号数</p>
<p>很抽象，这是道字符串题目，我思考的第一个版本一致WA$HDU 2054_1.CPP$ ，但是是比AC的$HDU 2054$考虑更加全面的</p>
<h3 id="HDU-2057"><a href="#HDU-2057" class="headerlink" title="HDU 2057"></a>HDU 2057</h3><p>这道题是计算16进制数的和。我觉得会是大数，就像使用字符串写。同号用while+stack实现出ans，异号先算出符号，再迭代+stack计算ans;ans去除前置0就是正真的答案。代码在HDU 2057.cpp 。结果是WA</p>
<p><a class="link"   href="https://blog.csdn.net/qq_40967787/article/details/89440209" >https://blog.csdn.net/qq_40967787/article/details/89440209<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="条件输入"><a href="#条件输入" class="headerlink" title="条件输入"></a>条件输入</h3><ul>
<li><p>cin&gt;&gt;a</p>
</li>
<li><p>scanf(“…”)&#x3D;&#x3D;EOF</p>
</li>
</ul>
<h3 id="printf和cout"><a href="#printf和cout" class="headerlink" title="printf和cout"></a>printf和cout</h3><ul>
<li>printf:非常适合指定小数点后位数的输出</li>
<li>cout:非常合适简单的输出</li>
</ul>
<h3 id="cin和scanf"><a href="#cin和scanf" class="headerlink" title="cin和scanf"></a>cin和scanf</h3><ul>
<li>cin:非常适合简单的输入，作为while的输入</li>
<li>scanf:非常适合有指定格式的输入</li>
</ul>
<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><p>set的抽象含义: 一个<strong>自动排序</strong>且<strong>不含重复元素</strong>的容器</p>
<p>set的访问: 仅支持迭代器的使用且不支持&lt; &gt;等元素符号</p>
<h3 id="HDU-2037和HDU-2021的共同点"><a href="#HDU-2037和HDU-2021的共同点" class="headerlink" title="HDU 2037和HDU 2021的共同点"></a>HDU 2037和HDU 2021的共同点</h3><p>贪心算法: </p>
<ul>
<li>局部最优导致全局最优解</li>
<li>预处理: 合理的排序</li>
<li>线性遍历即可完成决策</li>
</ul>
<h3 id="四舍五入-两位小数"><a href="#四舍五入-两位小数" class="headerlink" title="四舍五入(两位小数)"></a>四舍五入(两位小数)</h3><ul>
<li>printf(“%..f”)</li>
<li>int((a*100+0.5)&#x2F;100)</li>
<li>&lt;&lt; setprecision(2) &lt;&lt;</li>
</ul>
]]></content>
      <tags>
        <tag>算法刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>Mathematical Model:Summary</title>
    <url>/2025/05/11/Mathematical-Model-Summary/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于正在学习数学模型，稍微记录一些总结。</p>
<h2 id="常用工具总结"><a href="#常用工具总结" class="headerlink" title="常用工具总结"></a>常用工具总结</h2><ul>
<li>数据分析: EXCEL SPSS Python R Sta</li>
<li>百度搜索指数</li>
</ul>
<h2 id="1-评价类模型"><a href="#1-评价类模型" class="headerlink" title="1. 评价类模型"></a>1. 评价类模型</h2><h3 id="1-1-整体框架"><a href="#1-1-整体框架" class="headerlink" title="1.1 整体框架"></a>1.1 整体框架</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="D:\github\Ttzs-Git.github.io\source_posts\Mathematical-Model-Summary\image-20250511195818535.png"
                      alt="image-20250511195818535"
                ></p>
<p>评价模型希望从一堆方案中找到一个方案。通过<strong>正向化和标准化</strong>数据，我们期望得到<strong>分数最高</strong>的方案。</p>
<p>我们关心的是目标层和准则层之间的<strong>权重</strong>，方案层和准则层之间的<strong>初始分数</strong>，最终得到评分。</p>
<h3 id="1-2-层次分析法–主观权重"><a href="#1-2-层次分析法–主观权重" class="headerlink" title="1.2 层次分析法–主观权重"></a>1.2 层次分析法–主观权重</h3><p>层次分析法在<strong>缺少数据</strong>的情况下，通过<strong>分而治之</strong>的思想，将<strong>两两比较</strong>得出<strong>权重或者分数</strong></p>
<p>比较重要的内容如下:</p>
<ul>
<li><strong>一致矩阵的判定与调整</strong></li>
<li><strong>通过四种方法计算权重</strong></li>
</ul>
<p>缺陷:</p>
<ul>
<li>仅支持少量的数据的比较</li>
<li>对拥有数据的指标比较不准确</li>
</ul>
<h3 id="1-3-熵权法—客观权重"><a href="#1-3-熵权法—客观权重" class="headerlink" title="1.3 熵权法—客观权重"></a>1.3 熵权法—客观权重</h3><p>熵权法根据已有的数据，利用数据分布，计算信息熵，再归一化为权重</p>
<p>比较重要的内容:</p>
<ul>
<li>标准化化的方式</li>
<li>计算信息熵</li>
</ul>
<h3 id="1-4-TOPSIS法—计算得分"><a href="#1-4-TOPSIS法—计算得分" class="headerlink" title="1.4 TOPSIS法—计算得分"></a>1.4 TOPSIS法—计算得分</h3><p>TOPSIS是在拥有数据的情况下，充分利用数据和权重，计算最大最小值距离的一种方法</p>
<p>关键内容如下:</p>
<ul>
<li>对数据的正向化处理</li>
<li>最大值最小距离的计算</li>
</ul>
<h3 id="1-5-模糊评价法—计算得分-隶属度"><a href="#1-5-模糊评价法—计算得分-隶属度" class="headerlink" title="1.5 模糊评价法—计算得分(隶属度)"></a>1.5 模糊评价法—计算得分(隶属度)</h3><p>模糊评价法是对于评语集的隶属度的计算框架。</p>
<p>关键内容如下: </p>
<ul>
<li>选择隶属度函数</li>
<li>给出权重</li>
</ul>
<h2 id="2-预测类模型"><a href="#2-预测类模型" class="headerlink" title="2. 预测类模型"></a>2. 预测类模型</h2><h3 id="2-1-前言"><a href="#2-1-前言" class="headerlink" title="2.1 前言"></a>2.1 前言</h3><h4 id="2-1-1-核心问题"><a href="#2-1-1-核心问题" class="headerlink" title="2.1.1 核心问题"></a>2.1.1 核心问题</h4><p>问题: 强调对<strong>未来数据的推断</strong></p>
<h4 id="2-1-2-整体比较"><a href="#2-1-2-整体比较" class="headerlink" title="2.1.2 整体比较"></a>2.1.2 整体比较</h4><table>
<thead>
<tr>
<th>模型类型</th>
<th>优点</th>
<th>缺点</th>
<th>使用情况</th>
</tr>
</thead>
<tbody><tr>
<td><strong>灰色模型</strong></td>
<td><strong>小样本</strong>可用、建模简单</td>
<td>精度有限、不适合复杂关系</td>
<td><strong>数据不足、趋势明显</strong>的时间序列</td>
</tr>
<tr>
<td><strong>回归模型</strong></td>
<td>解释性强、精度高</td>
<td>需大量数据、对异常值敏感</td>
<td>数据丰富、变量关系明确</td>
</tr>
<tr>
<td><strong>时间序列</strong></td>
<td></td>
<td></td>
<td>通过历史数据的<strong>时间依赖性</strong>预测未来值（如ARIMA、指数平滑等）f(t)</td>
</tr>
</tbody></table>
<blockquote>
<p>灰色模型属于时间序列</p>
</blockquote>
<h2 id="3-统计类模型"><a href="#3-统计类模型" class="headerlink" title="3. 统计类模型"></a>3. 统计类模型</h2><h3 id="3-1-前言"><a href="#3-1-前言" class="headerlink" title="3.1 前言"></a>3.1 前言</h3><ul>
<li>核心问题: 建立变量之间的关系</li>
</ul>
<h3 id="3-2-统计回归—建立关系-一对多"><a href="#3-2-统计回归—建立关系-一对多" class="headerlink" title="3.2 统计回归—建立关系(一对多)"></a>3.2 统计回归—建立关系(一对多)</h3><blockquote>
<p>回归是<strong>拟合的子集</strong>，但更强调<strong>统计意义</strong></p>
<p>强调统计推断，提供参数估计、置信区间、假设检验等结果</p>
</blockquote>
<p>包括线性回归、逻辑回归等，用于分析变量间的关系</p>
<h3 id="3-3-相关系数–衡量相关程度"><a href="#3-3-相关系数–衡量相关程度" class="headerlink" title="3.3 相关系数–衡量相关程度"></a>3.3 相关系数–衡量相关程度</h3><p>衡量两个变量之间的线性相关程度（如皮尔逊相关系数）</p>
<h3 id="3-4-典型相关分析–建立关系-多对多"><a href="#3-4-典型相关分析–建立关系-多对多" class="headerlink" title="3.4 典型相关分析–建立关系(多对多)"></a>3.4 典型相关分析–建立关系(多对多)</h3><p>分析两组变量间的整体相关性（多变量相关性分析）</p>
<h2 id="4-优化类模型"><a href="#4-优化类模型" class="headerlink" title="4. 优化类模型"></a>4. 优化类模型</h2><h3 id="4-1-前言"><a href="#4-1-前言" class="headerlink" title="4.1 前言"></a>4.1 前言</h3><h4 id="4-1-1-核心问题"><a href="#4-1-1-核心问题" class="headerlink" title="4.1.1 核心问题"></a>4.1.1 核心问题</h4><p>核心问题: 求解最值问题或者最佳选择的问题(数学规划和资源分配问题)</p>
<h4 id="4-1-2-基本步骤"><a href="#4-1-2-基本步骤" class="headerlink" title="4.1.2 基本步骤"></a>4.1.2 基本步骤</h4><p>$$<br>决策变量x:控制变量\<br>目标函数f(x): 优化目标\<br>约束条件g_i(x)≤0: 现实中的限制<br>$$</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>1️⃣</td>
<td>明确目标（利润最大？成本最小？）</td>
</tr>
<tr>
<td>2️⃣</td>
<td>建立决策变量（你能控制的东西）</td>
</tr>
<tr>
<td>3️⃣</td>
<td>写出目标函数（怎么衡量好坏）</td>
</tr>
<tr>
<td>4️⃣</td>
<td>添加约束（资源限制、条件限制）</td>
</tr>
<tr>
<td>5️⃣</td>
<td>选择合适的工具（图解、软件如 LINGO）</td>
</tr>
<tr>
<td>6️⃣</td>
<td>分析解的意义（影子价格、灵敏度分析）</td>
</tr>
</tbody></table>
<h3 id="4-2-单目标优化"><a href="#4-2-单目标优化" class="headerlink" title="4.2 单目标优化"></a>4.2 单目标优化</h3><h3 id="4-3-多目标优化"><a href="#4-3-多目标优化" class="headerlink" title="4.3 多目标优化"></a>4.3 多目标优化</h3><ul>
<li><p>加权组合成一个新目标，化为单目标规划</p>
</li>
<li><p>一个目标作为另一个目标的约束条件，解另一个目标的规划</p>
</li>
</ul>
<h2 id="5-其他"><a href="#5-其他" class="headerlink" title="5. 其他"></a>5. 其他</h2><h3 id="5-1-插值"><a href="#5-1-插值" class="headerlink" title="5.1 插值"></a>5.1 插值</h3><p>给出匹配所有数据的函数</p>
<h3 id="5-2-拟合"><a href="#5-2-拟合" class="headerlink" title="5.2 拟合"></a>5.2 拟合</h3><p>拟合数据的曲线和曲面，只提供数学层面的表达式</p>
<h3 id="5-3-PCA"><a href="#5-3-PCA" class="headerlink" title="5.3 PCA"></a>5.3 PCA</h3><p>减少变量，实现降维</p>
]]></content>
      <tags>
        <tag>数学模型</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2025/03/25/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> or you can ask me on <a class="link"   href="https://github.com/hexojs/hexo/issues" >GitHub<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>how to study 1</title>
    <url>/2025/03/26/how-to-study-1/</url>
    <content><![CDATA[<p>Actually,I don’t know how to study. When I study CS61A and meet some problems which I can’t solve it quickly, I will feel irritable. Perhaps for my characters, it is quick to study from 0 to 1 due to interest. But, the subsequent courses are difficult and hard to stay clam to study them. </p>
<p>So, I determined to change it by some ways and write here to be a reminder for me.</p>
<ul>
<li><p>First of all ,i shouldn’t care too much about whether to learn all the courses for perfectionism and choose what is suitable for me. <strong>What matters is that learn the knowledge all not the courses all</strong> .</p>
</li>
<li><p>Secondly,to improve the efficiency of learning , <strong>read the preview materials</strong> before studying specially the CS. If I can <strong>finish the homework or lab</strong> <strong>or discussion well</strong>, it means I can skip this lesson. If it is hard for me to read it ,why not listen to it obediently?</p>
</li>
<li><p>Thirdly, how to learn by book ,read it ,rewrite the codes,try to change it ,and finally finish the practices.</p>
</li>
</ul>
<p>Someone perhaps ask me if i still be unknown to this knowledge after learning the course. It is useful for us to ask GAI like QwQ and so on.</p>
<p>It is a good choice to select a major course until you learn it down.</p>
<p>Hope you and me have a good studying experience.</p>
<p>Good luck! </p>
]]></content>
      <tags>
        <tag>Misc-talk</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2025/05/07/test/</url>
    <content><![CDATA[<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>博客优化</title>
    <url>/2025/05/07/%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="CDN"><a href="#CDN" class="headerlink" title="CDN"></a>CDN</h3><p>要求: 在该状态下希望使用免费的CDN</p>
<p>参考内容:</p>
<p><a class="link"   href="https://adaning.github.io/posts/42790.html#toc-heading-9" >Hexo博客优化<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><a class="link"   href="https://www.jsdelivr.com/" >jsDelivr<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="图片插入"><a href="#图片插入" class="headerlink" title="图片插入"></a>图片插入</h3><h4 id="一般文章的Typora的图片插入"><a href="#一般文章的Typora的图片插入" class="headerlink" title="一般文章的Typora的图片插入"></a>一般文章的Typora的图片插入</h4><p>参考文献:</p>
<p><a class="link"   href="https://yunjic.cn/2023/07/29/Hexo%E6%9B%B4%E4%BC%98%E9%9B%85%E7%9A%84%E5%9B%BE%E7%89%87%E6%8F%92%E5%85%A5%E6%96%B9%E5%BC%8F/" >Hexo更优雅的图片插入方式<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h4 id="网络地址logo图片"><a href="#网络地址logo图片" class="headerlink" title="网络地址logo图片"></a>网络地址logo图片</h4>]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>建站历程1</title>
    <url>/2025/05/07/%E5%BB%BA%E7%AB%99%E5%8E%86%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="历程"><a href="#历程" class="headerlink" title="历程"></a>历程</h2><p>我从去年12月左右开始建站，兜兜转转，中途放弃又捡起，放弃又捡起。</p>
<p>我原本的域名也被侵占了。</p>
<p>但我现在不由得感概，终于建好了。</p>
<p>我参考了很多文献，关于hexo，redefine，可能对大家有帮助，我放在文章最后作为查询集合</p>
<p>在建站的过程中，遇到了不少问题，其中很多其实有点记不清是如何解决的，后面完善站点的过程中再补充。</p>
<h2 id="文献集合"><a href="#文献集合" class="headerlink" title="文献集合"></a>文献集合</h2><ol>
<li><a class="link"   href="https://hexo.io/zh-cn/docs/" >Hexo-官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   href="https://redefine-docs.ohevan.com/zh" >Redefine-官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   href="https://catisnotfound.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" >catisnotfound<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   href="https://anemone.moe/posts/ultimate-guide-to-set-up-a-basic-personal-blog/" >主要参考文献—面向零基础小白的博客搭建教程<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   href="https://chat.qwen.ai/" >最重要的还有AI<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>…</li>
</ol>
<p>还有，但是记不得了，以后再补</p>
]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>CS25 v4-Hyung Won Chung</title>
    <url>/2025/09/25/CS25%20v4-Hyung%20Won%20Chung/</url>
    <content><![CDATA[<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>Hyung的lecture很有意思，分享了他<strong>预测AI未来发展趋势的方法</strong>，使我想起了李沐老师分享”<strong>从论文中寻找研究人员看待世界的角度</strong>“。他的核心观点是<strong>从主要驱动力的角度审视模型的一些归纳假设</strong>。从他跟同学的互动中，他对于RLFH持正向观点。</p>
<h2 id="如何预测多因子事件的发展"><a href="#如何预测多因子事件的发展" class="headerlink" title="如何预测多因子事件的发展"></a>如何预测多因子事件的发展</h2><p>如何在当下社会(每年都有巨量论文 → 不可能面面俱到 → 要抓住底层驱动力)中生存？Hyung 提供了一种“回归历史、预测未来”的方法: </p>
<ul>
<li>发现主要驱动力</li>
<li>理解主要驱动力</li>
<li>预测未来的轨迹<blockquote>
<p>根源在于寻找每个事件背后的最主要的驱动力，那么AI发展背后的主要驱动力是什么？</p>
</blockquote>
</li>
<li>更便宜的计算力和更大规模的计算</li>
</ul>
<h2 id="AI发展的基本逻辑"><a href="#AI发展的基本逻辑" class="headerlink" title="AI发展的基本逻辑"></a>AI发展的基本逻辑</h2><ul>
<li>朴素直觉: <strong>让AI像人一样思考</strong>,AI是能够<strong>模仿人类的智能</strong></li>
<li>问题: 我们人类并不真正了解自己的学习背后的机理，导致了一些AI可能出现瓶颈</li>
<li>真正的方法: 使用<strong>更弱的模型假设</strong>+增加<strong>更多的数据</strong>和<strong>算力</strong>(使得模型能够自行探索)</li>
<li>阶段性困境: 在算力较弱的时期，较弱的模型假设无法发挥所有的能力，性能可能不如较强假设的模型效果好</li>
<li>启发: AI工作者<strong>可能应该寻找当下性能较弱的工作</strong>，但潜在更通用”的方法</li>
</ul>
<h2 id="归纳假设"><a href="#归纳假设" class="headerlink" title="归纳假设"></a>归纳假设</h2><p>回归Transformer的三种架构，探寻其中的归纳假设</p>
<blockquote>
<p>这也是我当初学习时，一直好奇的，为什么Transformer中的经典架构，在后续的BERT和GPT3中只保留了部分</p>
</blockquote>
<table>
<thead>
<tr>
<th>模型架构</th>
<th>Encoder-Decoder</th>
<th>Decoder-Only</th>
<th>Encoder-Only</th>
</tr>
</thead>
<tbody><tr>
<td>代表</td>
<td>Transformer</td>
<td>GPT3</td>
<td>Bert</td>
</tr>
<tr>
<td>任务</td>
<td>机器翻译</td>
<td>生成任务</td>
<td>分类问题</td>
</tr>
<tr>
<td>额外的交叉注意力</td>
<td>分离的交叉注意力</td>
<td></td>
<td></td>
</tr>
<tr>
<td>参数共享</td>
<td>输入和输出的参数不同</td>
<td>共享参数</td>
<td></td>
</tr>
<tr>
<td>“目标到输入”的注意力模式</td>
<td>只在承担最后一层的encoder的输出</td>
<td>任意层任可直接关注目标</td>
<td></td>
</tr>
<tr>
<td>输入的注意机制</td>
<td>双向注意力</td>
<td>单向注意力(掩码机制)</td>
<td>双向注意力(专注输入编码，不做生成)</td>
</tr>
<tr>
<td>应用场景</td>
<td><strong>适合输入&#x2F;输出差异大的任务</strong></td>
<td><strong>强调序列生成与延续性</strong></td>
<td><strong>偏向语义理解与判别</strong></td>
</tr>
<tr>
<td>Encoder-Decoder:</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<ol>
<li>输入和输入相差甚远&#x3D;&gt; 使用了不同的架构处理输入和输出</li>
<li>目标项能承担全部输入的全部编码</li>
<li>当编码输入的序列时，在序列项中all-to-all是更好的</li>
</ol>
<p>除此之外，还提供其他的归纳假设的看法</p>
<ul>
<li><strong>任务架构 ≠ 核心瓶颈</strong>: 任务架构不是最重要的瓶颈限制，真正重要的是算力与弱假设模型的结合</li>
<li><strong>监督学习的局限</strong>: 监督学习可能过度限制，标签相当于人强加了额外假设，使得模型丧失了部分学习能力</li>
<li><strong>RLHF 的优势</strong>:RLHF使用更少的假设可能更利于模型</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] # Stanford CS25: V4 I Jason Wei &amp; Hyung Won Chung of OpenAI.<a class="link"   href="https://www.youtube.com/watch?v=3gb-ZkVRemQ&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27" >https://www.youtube.com/watch?v=3gb-ZkVRemQ&amp;list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&amp;index=27<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>Lecture</tag>
      </tags>
  </entry>
  <entry>
    <title>视觉基础模型综述</title>
    <url>/2025/09/24/%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] On the Opportunities and Risks of Foundation Models. <a class="link"   href="https://arxiv.org/pdf/2108.07258.pdf" >https://arxiv.org/pdf/2108.07258.pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><br>[2] 哈工大 SCIR. <a class="link"   href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&action=getalbum&album_id=2065548305387847684&scene=126&sessionid=1756366416082#wechat_redirect" >https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&amp;action=getalbum&amp;album_id=2065548305387847684&amp;scene=126&amp;sessionid=1756366416082#wechat_redirect<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>机器人学基础模型综述</title>
    <url>/2025/09/24/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] On the Opportunities and Risks of Foundation Models. <a class="link"   href="https://arxiv.org/pdf/2108.07258.pdf" >https://arxiv.org/pdf/2108.07258.pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><br>[2] 哈工大 SCIR. <a class="link"   href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&action=getalbum&album_id=2065548305387847684&scene=126&sessionid=1756366416082#wechat_redirect" >https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&amp;action=getalbum&amp;album_id=2065548305387847684&amp;scene=126&amp;sessionid=1756366416082#wechat_redirect<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>通用无线感知的调查-无线感知综述</title>
    <url>/2025/09/24/%E9%80%9A%E7%94%A8%E6%97%A0%E7%BA%BF%E6%84%9F%E7%9F%A5%E7%9A%84%E8%B0%83%E6%9F%A5-%E6%97%A0%E7%BA%BF%E6%84%9F%E7%9F%A5%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h2 id="哲学"><a href="#哲学" class="headerlink" title="哲学"></a>哲学</h2><ul>
<li>通过分析输入信号和接受到的无线感知信号的差异，得出信息</li>
<li>本质原理: 无线信号易受影响</li>
<li>流程: 环境设置、信号预处理、特征学习、模型部署</li>
</ul>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><ul>
<li>泛在性很差<ul>
<li>设备的异构化</li>
<li>人类身体形状的多样性</li>
<li>环境多样性</li>
</ul>
</li>
</ul>
<h2 id="增强手段"><a href="#增强手段" class="headerlink" title="增强手段"></a>增强手段</h2>]]></content>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>{{title}}</title>
    <url>/2025/09/25/template/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="📑"><a href="#📑" class="headerlink" title="📑 "></a>📑 {{title}}</h1><h2 id="1-基本信息"><a href="#1-基本信息" class="headerlink" title="1. 基本信息"></a>1. 基本信息</h2><ul>
<li><strong>作者 &#x2F; 单位</strong>:  </li>
<li><strong>会议 &#x2F; 期刊</strong>:  </li>
<li><strong>年份</strong>:</li>
</ul>
<hr>
<h2 id="2-研究背景"><a href="#2-研究背景" class="headerlink" title="2. 研究背景"></a>2. 研究背景</h2><ul>
<li><strong>研究问题</strong>:  </li>
<li><strong>应用场景</strong>:  </li>
<li><strong>现有方法不足</strong>:</li>
</ul>
<hr>
<h2 id="3-论文贡献-✅"><a href="#3-论文贡献-✅" class="headerlink" title="3. 论文贡献 ✅"></a>3. 论文贡献 ✅</h2><ul>
<li><input disabled="" type="checkbox"> 贡献 1:  </li>
<li><input disabled="" type="checkbox"> 贡献 2:  </li>
<li><input disabled="" type="checkbox"> 贡献 3:</li>
</ul>
<hr>
<h2 id="4-方法（Approach-System-Design）"><a href="#4-方法（Approach-System-Design）" class="headerlink" title="4. 方法（Approach &#x2F; System Design）"></a>4. 方法（Approach &#x2F; System Design）</h2><h3 id="📌-框架图"><a href="#📌-框架图" class="headerlink" title="📌 框架图"></a>📌 框架图</h3><h3 id="输入-输出"><a href="#输入-输出" class="headerlink" title="输入 &#x2F; 输出"></a>输入 &#x2F; 输出</h3><ul>
<li>输入:  </li>
<li>输出:</li>
</ul>
<h3 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h3><ul>
<li></li>
</ul>
<h3 id="特殊设计"><a href="#特殊设计" class="headerlink" title="特殊设计"></a>特殊设计</h3><ul>
<li></li>
</ul>
<hr>
<h2 id="5-实验与结果（Experiments-Results）"><a href="#5-实验与结果（Experiments-Results）" class="headerlink" title="5. 实验与结果（Experiments &amp; Results）"></a>5. 实验与结果（Experiments &amp; Results）</h2><ul>
<li><strong>数据集 &#x2F; 场景</strong>:  </li>
<li><strong>对比对象 (Baselines)</strong>:  </li>
<li><strong>评价指标</strong>:  </li>
<li><strong>结果总结</strong>:</li>
</ul>
<hr>
<h2 id="6-论文结论"><a href="#6-论文结论" class="headerlink" title="6. 论文结论"></a>6. 论文结论</h2><ul>
<li><strong>主要发现</strong>:  </li>
<li><strong>意义</strong>:  </li>
<li><strong>局限性</strong>:  </li>
<li><strong>未来方向</strong>:</li>
</ul>
<hr>
<h2 id="7-自己的思考-✍️"><a href="#7-自己的思考-✍️" class="headerlink" title="7. 自己的思考 ✍️"></a>7. 自己的思考 ✍️</h2><ul>
<li>和我研究 &#x2F; 项目的关系:  </li>
<li>可借鉴的点:  </li>
<li>改进思路:</li>
</ul>
]]></content>
      <tags>
        <tag>AIoT</tag>
        <tag>Paper</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title>语言基础模型综述</title>
    <url>/2025/09/24/%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] On the Opportunities and Risks of Foundation Models. <a class="link"   href="https://arxiv.org/pdf/2108.07258.pdf" >https://arxiv.org/pdf/2108.07258.pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><br>[2] 哈工大 SCIR. <a class="link"   href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&action=getalbum&album_id=2065548305387847684&scene=126&sessionid=1756366416082#wechat_redirect" >https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxMjAzNDY5Mg==&amp;action=getalbum&amp;album_id=2065548305387847684&amp;scene=126&amp;sessionid=1756366416082#wechat_redirect<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>CS25 v4-Jason Wei</title>
    <url>/2025/09/25/CS25%20v4-Jason%20Wei/</url>
    <content><![CDATA[<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>Jason主要围绕什么使得大语言模型如此奏效展开，核心观点是:<strong>大语言模型的成功并不是因为某一个单独的技术突破，而是多因素叠加的结果</strong> ，其中最有意思是<strong>scaling curves(缩放定律)</strong>。<br>	<strong>什么使得大语言模型如此奏效？</strong></p>
<ul>
<li>手动检查数据:我的理解是高质量的训练数据<ul>
<li><strong>手动检查与筛选的数据质量</strong>能够让模型更好地学习”规律”，而不是”模式匹配”</li>
</ul>
</li>
<li>多任务学习范式: 多任务学习范式使得LM适应多种任务(从翻译，生成摘要，到数学求解等)</li>
<li>计算机资源拓展: 大规模的计算促进损失下降<ul>
<li>更强的计算能力 → 可以训练更大的模型、更多的数据 → 训练损失整体下降</li>
</ul>
</li>
<li>全局的损失下降，但是部分的任务的损失可能上升<ul>
<li>部分任务在局部区间可能会“变差”，这是因为模型需要重新分配参数空间去学习更复杂的模式</li>
</ul>
</li>
<li>逆缩放曲线(Inverse Scaling Laws)<ul>
<li>增加数据量和模型规模，小范围内可能无法看到正确率的上升，反而下降</li>
<li>大模型并不是线性地“越来越好”，而是 <strong>跨越一个“理解门槛”</strong> 后，才会真正学会该任务</li>
</ul>
</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] # Stanford CS25: V4 I Jason Wei &amp; Hyung Won Chung of OpenAI.<a class="link"   href="https://www.youtube.com/watch?v=3gb-ZkVRemQ&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27" >https://www.youtube.com/watch?v=3gb-ZkVRemQ&amp;list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&amp;index=27<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
]]></content>
      <tags>
        <tag>Lecture</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态大模型入门指南-阅读提问</title>
    <url>/2025/09/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97-%E9%98%85%E8%AF%BB%E6%8F%90%E9%97%AE/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文是我在初步了解多模态大模型的过程中，结合博客阅读与GPT辅助理解后整理的文档。阅读本文档之前，您可以通过阅读第二部分的问题清单，判断您是否对于这些问题清楚</p>
<h1 id="问题清单"><a href="#问题清单" class="headerlink" title="问题清单"></a>问题清单</h1><p>下面的问题顺序是我阅读博客跟从GPT的对话中询问的问题，所以我是按照时间顺序整理的。因为我对于MMLLMs和LLM其实都不是很了解，所以会问很多基础问题、方法论问题以及抽象哲学问题。由于是问题的整理，所以在更高层面的问题会对小问题进行合并。<br>注意: 问题的颜色分类</p>
<blockquote>
<p>🔹 <strong>基础问题（入门级）</strong><br>   🔸 <strong>方法论问题（中阶）</strong><br>   🔺 <strong>抽象与哲学问题（高阶）</strong></p>
</blockquote>
<ol>
<li>🔹 是否可以认为传统 AI 更关注视觉、文本、图像等模态，而较少涉及如无线信号、语音、生理信号等多样化的感知模态？</li>
<li>🔹 信号的模态是不是比视觉&#x2F;文本&#x2F;图像更多样？</li>
<li>🔹 文中提到的 benchmark 是什么？</li>
<li>🔹 单模态基础模型是否只包括文字到文字的任务（如语言模型），还是也涵盖音频到音频、图像到图像等同模态映射任务？</li>
<li>🔸 Seq2Seq 模型与信号到信号（Signal-to-Signal）建模在本质上有什么区别？</li>
<li>🔸 Seq2Seq 框架下具体任务有哪些？</li>
<li>🔸 建模范式（Modeling Paradigms）可以如何划分？</li>
<li>🔺 能否更系统地抽象出不同模型思维方式或归纳假设之间的差异？</li>
<li>🔺常见的模型抽象有哪些？</li>
<li>🔺 是否存在比现有范式更高阶的抽象方式</li>
<li>🔸 多模态大模型（MM-LLM）可以从哪些维度进行系统级分类？</li>
<li>🔹 LLM 的 zero-shot 和 ICL 是什么？</li>
<li>🔹 LLM 中的预训练和指令微调分别是什么？</li>
<li>🔹 MMLLM 中输入和输出的 projector以及 LLM backbone 是什么？</li>
<li>🔹 ViT 中的 patch 是什么？图像被划分成 patch 后如何转化为 embedding？这种过程是否类似卷积？如果 patch 过大会不会丢失空间位置信息？</li>
<li>🔹 CLIP 是什么？除了 CLIP，对比学习还有哪些 case？</li>
<li>🔸 对比学习是否就是增强对角线元素、降低其他位置的值？</li>
<li>🔸 Modality Encoder 的输出是否就是 embedding？embedding 长度不一致所以需要 projector 吗？</li>
<li>🔸 其他模态的 signal token Sₓ 是什么？公式是什么意思？</li>
<li>🔸 MM IT（Instruction Tuning）是什么？</li>
<li>🔹 RLHF 之外还有哪些强化学习类型？</li>
<li>🔸 GAN 和 Multi-Agent RL 是否有关？</li>
<li>🔺 Transformer encoder-decoder 与 decoder-only 架构的归纳假设区别是什么？能否在算力和数据加持下用更简单的架构实现？</li>
<li>🔹 GPT-4V 是什么？</li>
<li>🔺 是否存在不是文本为中心，而是信号为核心的 MMLLM？</li>
</ol>
<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>基于上述产生的问题，经过查询资料和整理，形成下面的总结</p>
<h2 id="人工智能的整体认知"><a href="#人工智能的整体认知" class="headerlink" title="人工智能的整体认知"></a>人工智能的整体认知</h2><p>本部分主要介绍我当前对于人工智能的整体的认知。其中里面有很多还不是很清楚的内容，还需要经过实践的洗礼，不断的迭代升级。</p>
<h3 id="1-AI的发展阶段"><a href="#1-AI的发展阶段" class="headerlink" title="1. AI的发展阶段"></a>1. AI的发展阶段</h3><table>
<thead>
<tr>
<th align="center">层次</th>
<th align="left">核心范式</th>
<th align="left">智能特征</th>
<th align="left">推理形式</th>
<th align="left">代表方法&#x2F;模型</th>
<th align="left">哲学假设</th>
<th align="left">局限</th>
<th align="left">典型应用</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>统计AI</strong></td>
<td align="left">数据驱动（Pattern Learning）</td>
<td align="left">感知强、理解弱</td>
<td align="left">相关性 (Correlation)</td>
<td align="left">CNN, RNN, Transformer</td>
<td align="left">世界是统计模式的集合</td>
<td align="left">无法解释“为什么”</td>
<td align="left">图像识别、语音识别、NLP</td>
</tr>
<tr>
<td align="center"><strong>因果AI</strong></td>
<td align="left">模型驱动（Model Learning）</td>
<td align="left">能解释与预测</td>
<td align="left">干预 (Intervention)、反事实 (Counterfactual)</td>
<td align="left">Causal Graphs, SCM, Do-Calculus, Causal RL</td>
<td align="left">世界存在可描述的因果结构</td>
<td align="left">显式建模困难</td>
<td align="left">科学建模、决策支持</td>
</tr>
<tr>
<td align="center"><strong>交互式因果AI&#x2F;具身智能</strong></td>
<td align="left">环境交互（Embodied Learning）</td>
<td align="left">感知—行动一体化，具备直觉</td>
<td align="left">内隐因果 (Embodied Causality)</td>
<td align="left">具身智能体、世界模型、主动学习、自监督强化学习</td>
<td align="left">因果是行动中涌现的结构</td>
<td align="left">缺乏理论统一框架</td>
<td align="left">机器人、自主驾驶、交互体</td>
</tr>
<tr>
<td align="center">从表格中，我们能感受到AI越来越强调因果，强调逻辑，实现可解释性</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="2-AI的智能层级框架"><a href="#2-AI的智能层级框架" class="headerlink" title="2. AI的智能层级框架"></a>2. AI的智能层级框架</h3><p>我们期望构建一个<strong>正交、形式化、可解释</strong>的智能层级框架，以覆盖主流模型与方法</p>
<table>
<thead>
<tr>
<th>智能层级</th>
<th>核心问题</th>
<th>优化目标</th>
<th>数学对象</th>
<th>典型能力</th>
<th>对应 Pearl 因果之梯</th>
<th>智能</th>
<th>多模态领域下的解释</th>
</tr>
</thead>
<tbody><tr>
<td><strong>L1:关联（Seeing）</strong></td>
<td>“看到什么？”<br>变量间有何统计关联？</td>
<td>最小化预测误差</td>
<td>联合分布 $P(X_1, \dots, X_n)$ 或条件分布 $P(Y \mid X)$</td>
<td>模式识别、分类、预测</td>
<td>Seeing（观察）</td>
<td>感知</td>
<td>不同模态信息之间的关联性</td>
</tr>
<tr>
<td><strong>L2：生成（Imaging）</strong></td>
<td>“世界如何被构造？”<br>数据如何被生成？</td>
<td>最大似然或匹配分布</td>
<td>生成机制 $P(X) &#x3D; \int P(X \mid Z) P(Z) , dZ$，或隐变量模型</td>
<td>模拟、补全、想象、反事实推理</td>
<td>Imagining（反事实）</td>
<td>想象</td>
<td>跨模态生成与补全</td>
</tr>
<tr>
<td><strong>L3:干预(Doing)</strong></td>
<td>“如果我做某事，会发生什么？”<br>行动如何改变结果？</td>
<td>最大化干预下的期望回报</td>
<td>干预分布$ P(Y \mid \text{do}(X &#x3D; x))$，策略 $\pi(a \mid s)$</td>
<td>决策、控制、优化、实验设计</td>
<td>Doing（干预）</td>
<td>行动</td>
<td>跨模态策略与因果干预</td>
</tr>
<tr>
<td><strong>L4:表示(Abstracting)</strong></td>
<td>“世界的不变结构是什么？”<br>如何提取可迁移的语义？</td>
<td>提取不变、解耦、稀疏表示</td>
<td>不变特征映射 $\phi: X \to Z$，满足 $I(Z; Y) \gg I(X; Y)$ 且对扰动鲁棒</td>
<td>泛化、迁移、解耦、抽象（元认知能力）</td>
<td></td>
<td>抽象</td>
<td>跨模态不变语义空间构建</td>
</tr>
</tbody></table>
<p><em>形式化:统一框架下的建模</em><br>$$<br>\min_{f,\phi,\pi,g}\underbrace{\mathcal{L}<em>{\mathrm{assoc}}\left(f\circ\phi\right)}</em>{\mathrm{L1}}+\lambda_{1}\underbrace{\mathcal{L}<em>{\mathrm{gen}}\left(g,\phi\right)}</em>{\mathrm{L2}}+\lambda_{2}\underbrace{\mathcal{L}<em>{\mathrm{interv}}\left(\pi,f,g\right)}</em>{\mathrm{L3}}+\lambda_{3}\underbrace{\mathcal{R}<em>{\mathrm{rep}}\left(\phi\right)}</em>{\mathrm{L4}}<br>$$</p>
<p>$$<br> \begin{aligned}<br> &amp; \text{其中:} \<br> &amp; \bullet\quad\phi:\text{表示编码器(L}4) \<br> &amp; \bullet\quad f:\text{判别头(L}1) \<br> &amp; \bullet\quad g:\text{生成解码器(L2}) \<br> &amp; \bullet\quad\pi:\text{策略网络(L3}) \<br> &amp; \bullet\quad\mathcal{R}_{\mathrm{rep}}:\text{表示正则项(如不变性、稀疏性、解耦性})<br> \end{aligned}<br>$$<br><em>注意</em></p>
<ul>
<li><strong>建模范式是多维的</strong>，一个模型可同时属于多个范式。</li>
<li><strong>归纳偏置是模型泛化的关键</strong>，不同结构隐含不同世界假设。</li>
<li><strong>从关联 → 生成 → 干预 → 抽象</strong>，是智能建模的演进路径。</li>
<li><strong>统一框架</strong>有助于设计下一代通用人工智能系统（如具身多模态 Agent）<br><em>具身智能</em><br>具身智能是指智能体通过<strong>与物理环境持续交互</strong>、利用其<strong>身体结构、感知能力与行动机制</strong>，在<strong>感知—行动闭环</strong>中产生、适应与优化智能行为的系统。</li>
</ul>
<h3 id="3-自顶而下的智能框架"><a href="#3-自顶而下的智能框架" class="headerlink" title="3. 自顶而下的智能框架"></a>3. 自顶而下的智能框架</h3><div class="code-container" data-rel="Mathematica"><figure class="iseeu highlight mathematica"><table><tr><td class="code"><pre><span class="line">本体论层面（最高阶）</span><br><span class="line">└─ 问题本质：智能学习的任务是什么？</span><br><span class="line">   ├─ 表征世界 <span class="punctuation">(</span><span class="variable">Representation</span><span class="punctuation">)</span> → 如何编码信息？</span><br><span class="line">   ├─ 预测世界 <span class="punctuation">(</span><span class="variable">Prediction</span><span class="punctuation">)</span> → 能否从已知推测未知？</span><br><span class="line">   └─ 行动世界 <span class="punctuation">(</span><span class="variable">Action</span><span class="punctuation">)</span> → 能否通过行动改变未来？</span><br><span class="line"></span><br><span class="line">世界观层面（一阶抽象）</span><br><span class="line">└─ 问题本质：如何理解世界的生成与交互机制？</span><br><span class="line">	└─ 规则世界观 → 世界有边界规律（分类<span class="operator">/</span>回归）</span><br><span class="line">	└─ 生成世界观 → 世界是分布生成的（采样<span class="operator">/</span>生成）</span><br><span class="line">	└─ 干预世界观 → 世界可交互可干预（因果<span class="operator">/</span>强化）</span><br><span class="line"></span><br><span class="line">范式层面（二阶抽象）</span><br><span class="line">└─ 问题本质：如何形式化学习目标与结构假设？</span><br><span class="line">	├─ 判别式建模</span><br><span class="line">	├─ 生成式建模</span><br><span class="line">	├─ 序列建模</span><br><span class="line">	├─ 表示学习</span><br><span class="line">	├─ 对比学习</span><br><span class="line">	├─ 因果建模</span><br><span class="line">	└─ 强化学习</span><br><span class="line"></span><br><span class="line">架构与算法层面（三阶）</span><br><span class="line">└─ 问题本质：如何设计可计算的函数逼近器？</span><br><span class="line">	├─ <span class="variable">CNN</span></span><br><span class="line">	├─ <span class="variable">RNN</span></span><br><span class="line">	├─ <span class="variable">Transformer</span></span><br><span class="line">	├─ <span class="variable">Diffusion</span></span><br><span class="line">	├─ <span class="variable">GNN</span></span><br><span class="line">	└─ <span class="variable">MoE</span></span><br><span class="line"></span><br><span class="line">工程实践层面（最底层）</span><br><span class="line">└─ 问题本质：如何在现实约束下优化性能？</span><br><span class="line">	├─ 数据增强</span><br><span class="line">	├─ 预训练<span class="operator">-</span>微调范式</span><br><span class="line">	├─ <span class="variable">RLHF</span> <span class="operator">/</span> 人类反馈对齐</span><br><span class="line">	└─ 分布外鲁棒性</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>


<h3 id="4-常见建模范式（Modeling-Paradigms）的分类"><a href="#4-常见建模范式（Modeling-Paradigms）的分类" class="headerlink" title="4. 常见建模范式（Modeling Paradigms）的分类"></a>4. 常见建模范式（Modeling Paradigms）的分类</h3><p>建模范式是从不同维度对机器学习与人工智能模型进行抽象和分类的方式。单一模型往往同时属于多个范式（如 BERT 既是自监督、Transformer 结构、判别式、文本模态、Seq2Seq 框架下的语言模型）。下表从 <strong>六个正交维度</strong> 对主流范式进行划分：</p>
<table>
<thead>
<tr>
<th align="center">划分维度</th>
<th align="center">范式类别</th>
<th align="center">典型方法 &#x2F; 模型</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>学习监督方式</strong></td>
<td align="center">监督学习</td>
<td align="center">Logistic Regression, ResNet, BERT</td>
<td align="center">输入输出对明确，有标签<br> 目标是拟合标签（分类、回归）</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">无监督学习</td>
<td align="center">K-means, PCA, Autoencoder</td>
<td align="center">只靠输入数据学习结构（聚类、降维、密度估计、自编码器）</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">自监督学习</td>
<td align="center">Masked LM (BERT), SimCLR, MoCo</td>
<td align="center">用数据自身构造伪任务（掩码预测、对比学习、下一个 token 预测）无标签</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">半监督学习</td>
<td align="center">MixMatch, Semi-BERT</td>
<td align="center">小量标注 + 大量无标签</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">强化学习</td>
<td align="center">DQN, PPO, AlphaGo</td>
<td align="center">交互环境 + 奖励信号</td>
</tr>
<tr>
<td align="center"><strong>输入输出关系</strong></td>
<td align="center">点到点 (Point2Point)</td>
<td align="center">线性回归、MLP 分类</td>
<td align="center">单个输入 → 单个输出（x → y）</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">序列到点 (Seq2One)</td>
<td align="center">LSTM 做情感分类、Transformer 分类头</td>
<td align="center">输入序列 → 一个结果</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">序列到序列 (Seq2Seq)</td>
<td align="center">Transformer MT, T5, Whisper</td>
<td align="center">输入序列 → 输出序列</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">多模态到序列</td>
<td align="center">CLIP, Flamingo, GPT-4V</td>
<td align="center">图像&#x2F;信号 + 文本 → 序列</td>
</tr>
<tr>
<td align="center"><strong>模型结构 &#x2F; 归纳偏置</strong></td>
<td align="center">线性模型</td>
<td align="center">Linear Regression, Logistic</td>
<td align="center">无深度结构<br> 决策边界为超平面，特征线性组合</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">树模型</td>
<td align="center">Decision Tree, XGBoost</td>
<td align="center">常用于 tabular 数据<br>  分段常数、特征重要性、可解释性强</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">神经网络模型</td>
<td align="center">CNN(AlexNet, ResNet)</td>
<td align="center">擅长图像建模<br> <strong>局部性 + 平移不变性</strong>：邻近像素相关，模式可平移复用</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center">RNN(LSTM, GRU)</td>
<td align="center">早期序列建模<br><strong>时序依赖 + 递归状态</strong>：当前输出依赖历史状态</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center">Transformer(GPT, BERT, ViT)</td>
<td align="center">当前主流序列&#x2F;多模态<br> <strong>全局依赖 + 并行注意力</strong>：任意 token 可交互，无位置限制</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center">GNN(GCN, GraphSAGE)</td>
<td align="center">图结构建模<br><strong>图结构传播</strong>：节点信息通过邻居聚合</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">概率图模型</td>
<td align="center">HMM, CRF, Bayes Net</td>
<td align="center">统计建模<br> 显式建模变量间条件依赖关系</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">物理先验模型</td>
<td align="center">PINN（Physics-Informed Neural Network）</td>
<td align="center">融合物理定律作为约束</td>
</tr>
<tr>
<td align="center"><strong>优化目标 &#x2F; 损失</strong></td>
<td align="center">分类损失</td>
<td align="center">Cross-Entropy,Focal Loss</td>
<td align="center">监督分类(图像分类，文本分类)</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">回归损失</td>
<td align="center">MSE, MAE,Huber Loss</td>
<td align="center">监督回归(房价预测，目标检测坐标回归)</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">生成损失</td>
<td align="center">GAN Loss, Diffusion Loss,ELBO(变分推断)</td>
<td align="center">数据生成(图像生成、文本生成)</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">对比损失</td>
<td align="center">InfoNCE, Triplet Loss</td>
<td align="center">表示学习、多模态对齐</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">RL 损失</td>
<td align="center">Policy Gradient, Q-Learning</td>
<td align="center">强化学习(游戏AI,机器人控制)</td>
</tr>
<tr>
<td align="center"><strong>数据模态 &#x2F; 领域</strong></td>
<td align="center">文本</td>
<td align="center">GPT, BERT,T5</td>
<td align="center">语言模型: 语言建模、理解、生成</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">图像</td>
<td align="center">ResNet, ViT, Stable Diffusion</td>
<td align="center">视觉模型:分类、检测、生成、分割</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">音频&#x2F;语音</td>
<td align="center">Wav2Vec2, Whisper, TTS</td>
<td align="center">语音识别&#x2F;合成: ASR、TTS、语音情感识别</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">信号</td>
<td align="center">RadarNet, EEGNet</td>
<td align="center">通信、脑机接口</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">图结构</td>
<td align="center">GCN, GraphSAGE</td>
<td align="center">知识图谱、分子性质预测、推荐系统</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">多模态</td>
<td align="center">CLIP, Flamingo, MM-LLM</td>
<td align="center">跨模态对齐、检索、生成、推理</td>
</tr>
<tr>
<td align="center"><strong>学习&#x2F;推理方式</strong></td>
<td align="center">判别式</td>
<td align="center">Logistic Regression, BERT</td>
<td align="center">直接学习决策边界</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">生成式</td>
<td align="center">GPT, Diffusion, VAE</td>
<td align="center">建模数据分布并能采样</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">能量模型</td>
<td align="center">EBMs, Score Matching</td>
<td align="center">通过能量函数建模概率</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">对比学习</td>
<td align="center">SimCLR, CLIP</td>
<td align="center">通过相似性最大化构造表征空间</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">元学习</td>
<td align="center">MAML, ProtoNet</td>
<td align="center">学习如何学习</td>
</tr>
<tr>
<td align="center"><strong>输出任务</strong></td>
<td align="center">判别任务</td>
<td align="center">分类、回归</td>
<td align="center">输出类别&#x2F;数值(分类,回归)</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">生成任务</td>
<td align="center">文本生成、图像生成、信号生成</td>
<td align="center">输出新样本</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">检索任务</td>
<td align="center">CLIP embedding 检索</td>
<td align="center">输出相关样本</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">预测任务</td>
<td align="center">时间序列预测、轨迹预测</td>
<td align="center">输出未来值</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">控制任务</td>
<td align="center">机器人控制、RL</td>
<td align="center">输出动作</td>
</tr>
</tbody></table>
<h3 id="5-常见模型的归纳假设"><a href="#5-常见模型的归纳假设" class="headerlink" title="5.常见模型的归纳假设"></a>5.常见模型的归纳假设</h3><p>归纳假设是为了为了在有限算力和有限数据量的条件下，让模型能够从有限样本推广到无线可能的情况下的先验知识。<br>例如: </p>
<ul>
<li>CNN 假设 <strong>局部性 + 平移不变性</strong> → 图像中附近像素更相关，模式在空间上可以重复出现。</li>
<li>RNN 假设 <strong>时序依赖</strong> → 序列中前后的元素有因果关系，信息随时间流动。</li>
<li>Transformer 假设 <strong>全局依赖 + 并行注意</strong> → 任意两个 token 可能相关，不必局限于邻近元素。</li>
</ul>
<table>
<thead>
<tr>
<th><strong>建模类型</strong></th>
<th><strong>核心假设</strong></th>
<th><strong>思维方式 &#x2F; 方法</strong></th>
<th><strong>典型示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>判别式建模</strong></td>
<td>输入空间存在相对光滑、可分的决策边界</td>
<td>学习映射 (f(x) \to y)，关注类别可分性，不关心生成过程</td>
<td>逻辑回归、SVM、BERT（分类任务）</td>
</tr>
<tr>
<td><strong>生成式建模</strong></td>
<td>数据来自潜在分布，可通过模型重现</td>
<td>学习 (p(x)) 或 (p(y))，关注生成规律</td>
<td></td>
</tr>
<tr>
<td><strong>序列建模</strong></td>
<td>数据点有顺序，当前状态依赖历史</td>
<td>递推（RNN）、自注意力（Transformer）等编码依赖关系</td>
<td>语音建模、时间序列预测、语言建模</td>
</tr>
<tr>
<td><strong>对比式 &#x2F; 表示学习</strong></td>
<td>语义相近数据在潜在空间靠近，语义不相关远离</td>
<td>学习 embedding 空间，使相似性度量自然</td>
<td>SimCLR、CLIP、多模态对齐</td>
</tr>
<tr>
<td><strong>强化学习</strong></td>
<td>数据由交互生成，存在状态转移和奖励</td>
<td>最大化累积收益，而非拟合分布</td>
<td>AlphaGo、机器人控制</td>
</tr>
<tr>
<td><strong>因果建模</strong></td>
<td>统计相关性背后存在因果结构</td>
<td>构建结构方程模型 (SEM) 或通过干预推理</td>
<td>因果图模型、Do-calculus 应用于医疗&#x2F;经济预测</td>
</tr>
</tbody></table>
<h3 id="6-Seq2Seq-的工作和信号到信号的工作的区别"><a href="#6-Seq2Seq-的工作和信号到信号的工作的区别" class="headerlink" title="6. Seq2Seq 的工作和信号到信号的工作的区别"></a>6. Seq2Seq 的工作和信号到信号的工作的区别</h3><table>
<thead>
<tr>
<th align="center"><strong>维度</strong></th>
<th align="left"><strong>Seq2Seq（序列到序列）</strong></th>
<th align="left"><strong>Signal-to-Signal（信号到信号）</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>定义</strong></td>
<td align="left">输入一个序列，输出另一个序列</td>
<td align="left">输入一个原始信号序列，输出另一个信号序列</td>
</tr>
<tr>
<td align="center"><strong>典型任务</strong></td>
<td align="left">- 机器翻译（英语句子 → 中文句子）<br>- 自动摘要（长文档 → 短文摘要）<br>- 语音识别（音频特征序列 → 文本序列）</td>
<td align="left">- 音频增强（有噪音音频 → 干净音频）<br>- 语音合成（语音特征 → 波形）<br>- 无线通信（接收端信号 → 去噪&#x2F;解码信号）<br>- 脑电跨模态（EEG → fNIRS）</td>
</tr>
<tr>
<td align="center"><strong>特点</strong></td>
<td align="left">- 输入&#x2F;输出均为“序列”，模态可不同（文字、语音特征、动作编码等）<br>- 强调 <strong>时序结构</strong> 与 <strong>条件生成</strong></td>
<td align="left">- 输入&#x2F;输出通常为相同或相似模态（都是信号）<br>- 强调 <strong>保真度、信号还原、物理约束</strong></td>
</tr>
<tr>
<td align="center"><strong>范式 &#x2F; 应用关系</strong></td>
<td align="left">一种 <strong>建模范式</strong>（适用于任何序列 → 序列的任务）</td>
<td align="left">属于 <strong>具体应用场景</strong>，通常在 <strong>Seq2Seq 框架下实现</strong>，只是输入&#x2F;输出都是真实信号</td>
</tr>
</tbody></table>
<h3 id="7-Seq2Seq-框架下常见任务"><a href="#7-Seq2Seq-框架下常见任务" class="headerlink" title="7. Seq2Seq 框架下常见任务"></a>7. Seq2Seq 框架下常见任务</h3><div class="code-container" data-rel="Mathematica"><figure class="iseeu highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="variable">Seq2Seq</span> 框架 <span class="punctuation">(</span>序列输入 → 序列输出<span class="punctuation">)</span></span><br><span class="line">├─ <span class="punctuation">(</span><span class="variable">a</span><span class="punctuation">)</span> 文本相关</span><br><span class="line">│   ├─ 机器翻译：英文句子 → 中文句子</span><br><span class="line">│   ├─ 文本摘要：长文本 → 短摘要</span><br><span class="line">│   ├─ 对话生成：对话历史 → 下一句回复</span><br><span class="line">│</span><br><span class="line">├─ <span class="punctuation">(</span><span class="variable">b</span><span class="punctuation">)</span> 语音 <span class="operator">/</span> 音频相关</span><br><span class="line">│   ├─ 语音识别 <span class="punctuation">(</span><span class="variable">ASR</span><span class="punctuation">)</span>：音频序列 → 文本序列</span><br><span class="line">│   ├─ 语音合成 <span class="punctuation">(</span><span class="variable">TTS</span><span class="punctuation">)</span>：文本 → 语音波形</span><br><span class="line">│   ├─ 语音增强 <span class="punctuation">(</span><span class="variable">Speech</span> <span class="variable">Enhancement</span><span class="punctuation">)</span>：带噪音音频 → 干净音频</span><br><span class="line">│</span><br><span class="line">├─ <span class="punctuation">(</span><span class="variable">c</span><span class="punctuation">)</span> 视觉相关</span><br><span class="line">│   ├─ 图像描述 <span class="punctuation">(</span><span class="built_in">Image</span> <span class="variable">Captioning</span><span class="punctuation">)</span>：图像特征序列 → 文本序列</span><br><span class="line">│   ├─ 视频字幕生成：视频帧序列 → 文本</span><br><span class="line">│</span><br><span class="line">├─ <span class="punctuation">(</span><span class="variable">d</span><span class="punctuation">)</span> 信号相关</span><br><span class="line">│   ├─ 无线信号解码：接收信号序列 → 原始比特序列</span><br><span class="line">│   ├─ 跨模态信号转换：<span class="variable">EEG</span> 信号 → 语音；雷达信号 → 图像</span><br><span class="line">│   ├─ 时序预测：过去信号序列 → 未来信号序列（天气、股价预测）</span><br><span class="line">│</span><br><span class="line">└─ <span class="punctuation">(</span><span class="variable">e</span><span class="punctuation">)</span> 跨模态任务</span><br><span class="line">    ├─ 语音翻译：语音序列 → 不同语言的文本序列</span><br><span class="line">    ├─ 手势识别：<span class="variable">IMU</span> 序列 → 动作类别</span><br><span class="line">    └─ <span class="variable">IoT</span> 多模态感知：传感器信号序列 → 行为标签 <span class="operator">/</span> 语音 <span class="operator">/</span> 图像</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>


<h2 id="单模态基础模型的常见知识点"><a href="#单模态基础模型的常见知识点" class="headerlink" title="单模态基础模型的常见知识点"></a>单模态基础模型的常见知识点</h2><p>单模态基础模型是指”<strong>只处理单一模态数据</strong> 的大模型”,包含”语言单模态”，“视觉单模态”，“音频单模态”模型</p>
<h3 id="LLM-的-zero-shot-和-ICL-能力"><a href="#LLM-的-zero-shot-和-ICL-能力" class="headerlink" title="LLM 的 zero-shot 和 ICL 能力"></a>LLM 的 zero-shot 和 ICL 能力</h3><blockquote>
<p>原文”LLM 具有良好的语言生成，zero-shot 和 ICL 的能力”</p>
</blockquote>
<table>
<thead>
<tr>
<th><strong>学习模式</strong></th>
<th><strong>定义</strong></th>
<th><strong>核心逻辑</strong></th>
<th><strong>示例</strong></th>
<th><strong>本质</strong></th>
<th><strong>多模态应用示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Zero-shot Learning (零样本学习)</td>
<td>在没有针对任务的专门训练样本下，直接完成任务</td>
<td>模型在预训练阶段学到的通用知识和映射规则，可迁移到新任务</td>
<td>翻译“apple → pomme”，VQA回答“How many apples on the table?”</td>
<td>预训练阶段学到的通用映射规则被迁移到新任务</td>
<td>多模态 LLM 在未见过的 VQA 或图像描述任务中直接推理</td>
</tr>
<tr>
<td>In-Context Learning (上下文学习, ICL)</td>
<td>在推理阶段通过上下文示例学习完成任务，无需更新模型权重</td>
<td>利用输入上下文的示例，进行模式匹配，模仿“输入→输出映射”</td>
<td>给 LLM 翻译示例：cat→chat, dog→chien, house→maison → tree？模型输出 arbre</td>
<td>LLM 充当上下文函数拟合器，利用上下文快速迁移</td>
<td>给多模态 LLM 几对 &lt;图像, caption&gt;，生成新图像的 caption，无需微调</td>
</tr>
</tbody></table>
<h3 id="LLM-中的预训练和指令微调"><a href="#LLM-中的预训练和指令微调" class="headerlink" title="LLM 中的预训练和指令微调"></a>LLM 中的预训练和指令微调</h3><table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>预训练 (Pre-training)</strong></th>
<th><strong>指令微调 (Instruction Tuning)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>定义</strong></td>
<td>在大规模语料或多模态数据上进行通用任务训练，让模型获得广泛语言&#x2F;知识&#x2F;感知能力</td>
<td>在人工整理的任务指令数据上进行监督微调，使模型能理解并执行自然语言指令</td>
</tr>
<tr>
<td><strong>数据来源</strong></td>
<td>超大规模（互联网文本、图文对、视频字幕、音频转录等）</td>
<td>人工构造或过滤的高质量 (instruction, input, output) 三元组</td>
</tr>
<tr>
<td><strong>学习目标</strong></td>
<td>自监督学习（语言模型的 next-token prediction、图文对比学习、时间片段预测等）</td>
<td>监督学习（任务输入 → 输出）</td>
</tr>
<tr>
<td><strong>结果能力</strong></td>
<td>通才模型：语言理解、跨模态表示能力，通用知识</td>
<td>模型更“对话化”，能遵循指令完成具体任务</td>
</tr>
<tr>
<td><strong>数据规模</strong></td>
<td>TB 级</td>
<td>百万到千万级</td>
</tr>
<tr>
<td><strong>数据类型</strong></td>
<td>原始网络数据</td>
<td>人工清理过的任务数据</td>
</tr>
<tr>
<td><strong>类比</strong></td>
<td>上小学学“百科知识”</td>
<td>上辅导班学“答题技巧”</td>
</tr>
<tr>
<td><strong>典型例子</strong></td>
<td>GPT 系列、CLIP、Flamingo、BLIP-2</td>
<td>Alpaca、Vicuna、InstructGPT、LLaVA</td>
</tr>
</tbody></table>
<h2 id="多模态基础模型的常见知识点"><a href="#多模态基础模型的常见知识点" class="headerlink" title="多模态基础模型的常见知识点"></a>多模态基础模型的常见知识点</h2><h3 id="多模态模型的概要"><a href="#多模态模型的概要" class="headerlink" title="多模态模型的概要"></a>多模态模型的概要</h3><p>主流大模型主要集中在”视觉-语言”任务，评价体系也建立在大量视觉-语言benchmark。<br>“视觉-文本”任务的数据规模大，任务需求广。<br>信号模态由于缺少统一的benchmark和数据获取困难且任务场景分散从而研究得少<br>信号模态远比图像&#x2F;文本&#x2F;图像更要多样，分布涉及声学信号，电磁信号，生理信号，机械信号，环境信号，混合信号，多模态信号等</p>
<h3 id="多模态领域的benchmark"><a href="#多模态领域的benchmark" class="headerlink" title="多模态领域的benchmark"></a>多模态领域的benchmark</h3><p>Benchmark是一种公认的衡量体系(公认的数据集+评测任务形式+评价指标)。标准量化了模型的性能，便于不同研究之间的比较，推动发展。<br>例: 视觉-语言领域常见的benchmark</p>
<table>
<thead>
<tr>
<th><strong>任务&#x2F;数据集</strong></th>
<th><strong>输入</strong></th>
<th><strong>输出&#x2F;任务目标</strong></th>
<th><strong>任务形式</strong></th>
<th><strong>典型数据集</strong></th>
<th><strong>常用评价指标</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>VQAv2 (Visual Question Answering v2)</strong></td>
<td>一张图片 + 自然语言问题</td>
<td>模型回答问题（自然语言答案）</td>
<td><strong>判别 + 生成式问答</strong></td>
<td>VQAv2</td>
<td>Accuracy (整体&#x2F;开放式答案匹配)</td>
</tr>
<tr>
<td><strong>COCO Caption &#x2F; Flickr30k Captioning</strong></td>
<td>一张图片</td>
<td>生成自然语言描述</td>
<td><strong>生成式文本生成</strong></td>
<td>COCO Caption, Flickr30k</td>
<td>BLEU, METEOR, ROUGE, CIDEr, SPICE</td>
</tr>
<tr>
<td><strong>NLVR2 (Natural Language for Visual Reasoning 2)</strong></td>
<td>两张图片 + 一段文本</td>
<td>判断文本是否正确描述图片 (True&#x2F;False)</td>
<td><strong>判别式推理</strong></td>
<td>NLVR2</td>
<td>Accuracy</td>
</tr>
<tr>
<td><strong>RefCOCO &#x2F; RefCOCOg &#x2F; RefCOCO+</strong></td>
<td>一张图片 + 文本指令（目标描述）</td>
<td>在图片中定位目标物体（bounding box &#x2F; segmentation mask）</td>
<td><strong>定位&#x2F;检测任务</strong></td>
<td>RefCOCO, RefCOCOg, RefCOCO+</td>
<td>IoU (Intersection over Union), Accuracy</td>
</tr>
<tr>
<td><strong>ScienceQA &#x2F; TextVQA &#x2F; DocVQA</strong></td>
<td>图片（科学图表 &#x2F; 文档） + 文本问题</td>
<td>模型回答问题（自然语言答案）</td>
<td><strong>跨模态问答</strong></td>
<td>ScienceQA, TextVQA, DocVQA</td>
<td>Accuracy, Exact Match (EM)</td>
</tr>
<tr>
<td><strong>MMBench &#x2F; SEED-Bench &#x2F; LVLM-eHub</strong></td>
<td>多模态输入（图片&#x2F;文本等）</td>
<td>多任务统一测评（分类、问答、推理、描述等）</td>
<td><strong>综合基准评测</strong></td>
<td>MMBench, SEED-Bench, LVLM-eHub</td>
<td>多任务指标（Accuracy, BLEU, CIDEr, IoU 等，根据子任务）</td>
</tr>
</tbody></table>
<h3 id="MM-LLM的典型结构"><a href="#MM-LLM的典型结构" class="headerlink" title="MM-LLM的典型结构"></a>MM-LLM的典型结构</h3><div class="code-container" data-rel="Mathematica"><figure class="iseeu highlight mathematica"><table><tr><td class="code"><pre><span class="line">输入模态  →  模态编码器<span class="punctuation">(</span>预处理特征<span class="punctuation">)</span>  </span><br><span class="line">          →  对齐模块<span class="punctuation">(</span><span class="built_in">Projection</span> <span class="operator">/</span> <span class="variable">Adapter</span> <span class="operator">/</span> <span class="variable">Attention</span><span class="punctuation">)</span>  </span><br><span class="line">          →  融合<span class="operator">/</span>交互模块<span class="punctuation">(</span><span class="variable">MM</span><span class="operator">-</span><span class="variable">Transformer</span> <span class="operator">/</span> <span class="built_in">Cross</span><span class="operator">-</span><span class="variable">Attn</span><span class="punctuation">)</span>  </span><br><span class="line">          →  <span class="variable">LLM</span> <span class="punctuation">(</span>语言推理<span class="operator">/</span>生成<span class="punctuation">)</span>  </span><br><span class="line">          →  下游任务输出</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<p>![[Pasted image 20251004181619.jpg]]</p>
<h4 id="Modality-Encoder"><a href="#Modality-Encoder" class="headerlink" title="Modality Encoder"></a>Modality Encoder</h4><ul>
<li>功能:<ul>
<li>编码不同模态的输入获取特征</li>
</ul>
</li>
<li>输出:<ul>
<li>不同维度，语音分布不一致、位置编码异构的embedding</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center"><strong>模态类型</strong></th>
<th align="left"><strong>编码器名称</strong></th>
<th align="left"><strong>核心结构&#x2F;机制</strong></th>
<th align="left"><strong>训练范式</strong></th>
<th align="left"><strong>主要特征与优势</strong></th>
<th align="left"><strong>典型用途</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>视觉（图像）</strong></td>
<td align="left"><strong>NFNet-F6</strong></td>
<td align="left">无归一化的 ResNet 架构（No Normalization ResNet）</td>
<td align="left">有监督</td>
<td align="left">- 去掉 BatchNorm 提高数值稳定性- 在增强数据集上达 SOTA</td>
<td align="left">高精度图像分类</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>ViT (Vision Transformer)</strong></td>
<td align="left">将图像切分为 patch，经线性投影后输入 Transformer</td>
<td align="left">有监督 &#x2F; 自监督</td>
<td align="left">- 全局建模- 可迁移到下游任务</td>
<td align="left">图像识别、视觉编码</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>CLIP-ViT</strong></td>
<td align="left">ViT + 文本编码器；跨模态对比学习</td>
<td align="left">对比学习（Image-Text）</td>
<td align="left">- 学习视觉与语言的统一语义空间- 强大的零样本能力</td>
<td align="left">图文对齐、跨模态检索</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>EVA-CLIP ViT</strong></td>
<td align="left">对 CLIP 进行优化与扩展</td>
<td align="left">对比学习（Image-Text）</td>
<td align="left">- 稳定大规模 CLIP 训练- 提升视觉编码质量与鲁棒性</td>
<td align="left">大规模多模态预训练</td>
</tr>
<tr>
<td align="center"><strong>视觉（视频）</strong></td>
<td align="left">同上（统一采样约 5 帧）</td>
<td align="left">对视频帧进行与图像相同的编码处理</td>
<td align="left">—</td>
<td align="left">- 简化视频编码流程- 可迁移图像预训练模型</td>
<td align="left">视频理解、视频-文本对齐</td>
</tr>
<tr>
<td align="center"><strong>音频</strong></td>
<td align="left"><strong>C-Former</strong></td>
<td align="left">CIF（Continuous Integrate-and-Fire）对齐 + Transformer</td>
<td align="left">自监督 &#x2F; 弱监督</td>
<td align="left">- 实现时序对齐与序列建模- 精确时间同步</td>
<td align="left">语音识别、语音对齐</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>HuBERT</strong></td>
<td align="left">BERT 架构 + Mask 预测 + 隐单元离散化</td>
<td align="left">自监督</td>
<td align="left">- 利用未标注音频学习隐藏特征- 双向上下文建模</td>
<td align="left">语音表示学习</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>BEATs</strong></td>
<td align="left">双向音频 Transformer + 迭代式预训练</td>
<td align="left">自监督</td>
<td align="left">- 统一多任务音频表示- 高效迁移到多下游任务</td>
<td align="left">音频分类、事件检测</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>Whisper</strong></td>
<td align="left">Encoder-Decoder Transformer</td>
<td align="left">监督 &#x2F; 弱监督</td>
<td align="left">- 端到端语音识别 + 翻译- 多语言、多任务鲁棒性</td>
<td align="left">语音识别、语音翻译</td>
</tr>
</tbody></table>
<h4 id="Input-Projector"><a href="#Input-Projector" class="headerlink" title="Input Projector"></a>Input Projector</h4><ul>
<li>功能: 模态对齐<ul>
<li>把不同模态的特征对齐到LLM的embedding空间</li>
<li>保证输入到 LLM 的模态特征与文本 token embedding同维度、同分布</li>
</ul>
</li>
</ul>
<h4 id="LLM-Backbone"><a href="#LLM-Backbone" class="headerlink" title="LLM Backbone"></a>LLM Backbone</h4><ul>
<li>功能:<ul>
<li>以语言为核心进行跨模态的理解和推理，以及最后的输出</li>
</ul>
</li>
</ul>
<h4 id="Output-Projector"><a href="#Output-Projector" class="headerlink" title="Output Projector"></a>Output Projector</h4><ul>
<li>功能:<ul>
<li>把LLM的输出token embedding 转化为目标模态的特征空间</li>
<li>如果任务是 <strong>跨模态生成</strong>，比如“文字 → 图像”，必须有输出 Projector</li>
</ul>
</li>
</ul>
<h4 id="Modality-Generator"><a href="#Modality-Generator" class="headerlink" title="Modality Generator"></a>Modality Generator</h4><ul>
<li>功能:<ul>
<li>生成不同的模态作为输出</li>
</ul>
</li>
</ul>
<h4 id="归纳假设"><a href="#归纳假设" class="headerlink" title="归纳假设"></a>归纳假设</h4><ol>
<li><strong>模态独立假设</strong>（Modality-specific representations）<ul>
<li>每种模态（图像、视频、音频）都有其专门的 Encoder 来学习表示 → 假设不同模态需要独立的表征空间。</li>
</ul>
</li>
<li><strong>表征可对齐假设</strong>（Alignable representations）<ul>
<li>经过 <strong>Input Projector</strong>（比如 MLP、Cross-Attention、Q-Former），不同模态的表示可以投影&#x2F;对齐到一个共享语义空间 → 假设所有模态都能在统一的语义空间里交流。</li>
</ul>
</li>
<li><strong>语言中心假设</strong>（Language as backbone）<ul>
<li>LLM Backbone 是整个智能的核心 → 假设语言是最好的“中介”来承载推理、理解、生成能力。</li>
</ul>
</li>
<li><strong>条件生成假设</strong>（Conditional generation）<ul>
<li>生成端依赖 <strong>Output Projector + 模态生成器 (MG_X)</strong>，即假设“任何模态都可以看作是语言条件下的条件生成”。</li>
</ul>
</li>
</ol>
<div class="code-container" data-rel="Mathematica"><figure class="iseeu highlight mathematica"><table><tr><td class="code"><pre><span class="line">训练<span class="operator">:</span> 在训练过程中，通常<span class="operator">**</span>冻结 <span class="variable">LLM</span> <span class="variable">Backbone</span>、模态编码器与模态生成器<span class="operator">**</span>，仅训练投影模块（<span class="built_in">Input</span><span class="operator">/</span><span class="variable">Output</span> <span class="variable">Projector</span>）以实现<span class="operator">**</span>模态对齐<span class="operator">**</span>。</span><br></pre></td></tr></table></figure></div>
<h3 id="Embedding-Token-对齐"><a href="#Embedding-Token-对齐" class="headerlink" title="Embedding &amp; Token &amp; 对齐"></a>Embedding &amp; Token &amp; 对齐</h3><table>
<thead>
<tr>
<th>角度</th>
<th><strong>Token</strong></th>
<th><strong>Embedding</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>本质</strong></td>
<td>离散符号（symbol）</td>
<td>连续向量（vector）</td>
</tr>
<tr>
<td><strong>信息形式</strong></td>
<td>符号化信息（Symbolic）</td>
<td>几何化信息（Geometric）</td>
</tr>
<tr>
<td><strong>可解释性</strong></td>
<td>强（人类可读：词&#x2F;子词）</td>
<td>弱（语义分布在向量结构中）</td>
</tr>
<tr>
<td><strong>计算操作</strong></td>
<td>只能索引或判等（&#x3D;&#x3D;）</td>
<td>可进行点积、余弦相似度、线性变换等</td>
</tr>
<tr>
<td><strong>学习方式</strong></td>
<td>预定义（由 tokenizer 确定）</td>
<td>可学习（通过梯度优化得到）</td>
</tr>
<tr>
<td><strong>来源</strong></td>
<td>Tokenizer 输出（词表索引）</td>
<td>Embedding 层（投影为向量）</td>
</tr>
<tr>
<td><strong>是否可直接输入模型</strong></td>
<td>❌ 否（需映射为 embedding）</td>
<td>✅ 是（模型操作对象）</td>
</tr>
<tr>
<td><strong>可学习性</strong></td>
<td>固定</td>
<td>可学习</td>
</tr>
<tr>
<td><strong>CLIP 对齐的对象</strong></td>
<td>❌ 否</td>
<td>✅ 是（在语义空间对齐）</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>维度</th>
<th><strong>Token-level 对齐</strong></th>
<th><strong>Embedding-level 对齐</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>表示形式</strong></td>
<td>离散符号（symbolic）</td>
<td>连续向量（continuous）</td>
</tr>
<tr>
<td><strong>优化方式</strong></td>
<td>基于显式匹配（如 attention 对齐、region-word 对齐）</td>
<td>基于相似性度量（如余弦距离、对比学习）</td>
</tr>
<tr>
<td><strong>数据依赖</strong></td>
<td>需强标注（词 ↔ 区域）</td>
<td>弱标注即可（整体图 ↔ 文本描述）</td>
</tr>
<tr>
<td><strong>训练稳定性</strong></td>
<td>差（符号难以优化）</td>
<td>好（连续空间可微分）</td>
</tr>
<tr>
<td><strong>泛化能力</strong></td>
<td>弱（依赖具体模态结构）</td>
<td>强（依赖语义结构）</td>
</tr>
<tr>
<td><strong>代表模型</strong></td>
<td>ViLBERT, LXMERT（BERT + Faster R-CNN）</td>
<td>CLIP, ALIGN, BLIP-2, ImageBind</td>
</tr>
<tr>
<td><strong>难易程度</strong></td>
<td>🚫 难（高成本、低扩展）</td>
<td>✅ 容易（主流选择）</td>
</tr>
</tbody></table>
<h3 id="MM-LLM的预训练和指令微调"><a href="#MM-LLM的预训练和指令微调" class="headerlink" title="MM-LLM的预训练和指令微调"></a>MM-LLM的预训练和指令微调</h3><p>IT的主流范式</p>
<table>
<thead>
<tr>
<th>维度</th>
<th><strong>SFT（Supervised Fine-Tuning）</strong></th>
<th><strong>RLHF（Reinforcement Learning with Human Feedback）</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>中文名称</strong></td>
<td>监督微调</td>
<td>人类反馈强化学习</td>
</tr>
<tr>
<td><strong>核心目标</strong></td>
<td>学会遵循人类指令（Instruction Following）</td>
<td>学会符合人类偏好（Preference Alignment）</td>
</tr>
<tr>
<td><strong>数据来源</strong></td>
<td>人工标注的“指令 → 响应”对</td>
<td>人类对多候选输出的偏好打分</td>
</tr>
<tr>
<td><strong>优化思路</strong></td>
<td>直接最小化监督损失（如交叉熵）</td>
<td>间接最大化人类偏好（通过奖励模型）</td>
</tr>
<tr>
<td><strong>训练信号</strong></td>
<td>显式目标输出（Ground Truth）</td>
<td>奖励信号（Reward from preference）</td>
</tr>
<tr>
<td><strong>学习方式</strong></td>
<td>监督学习（Supervised Learning）</td>
<td>强化学习（Reinforcement Learning）</td>
</tr>
<tr>
<td><strong>典型流程</strong></td>
<td>模型输入(图像&#x2F;文本) → 生成 → 与人工答案对齐</td>
<td>模型生成多个输出 → 人类排序 → 奖励模型学习 → 优化策略</td>
</tr>
<tr>
<td><strong>典型算法&#x2F;方法</strong></td>
<td>Cross-Entropy Loss, Instruction Tuning</td>
<td>PPO (Proximal Policy Optimization), DPO</td>
</tr>
<tr>
<td><strong>代表模型阶段</strong></td>
<td>ChatGPT（SFT阶段）, LLaVA, BLIP-2</td>
<td>ChatGPT（RLHF阶段）, GPT-4, Gemini, Qwen2-VL</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>稳定、简单、高效，易于扩展</td>
<td>提升模型价值对齐与人类偏好匹配度</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>易过拟合、缺乏灵活性</td>
<td>训练复杂、成本高、稳定性差</td>
</tr>
<tr>
<td><strong>简要例子</strong></td>
<td>给图像+问题 → 模型输出正确答案（GT）</td>
<td>给图像+指令 → 模型生成3个答案 → 人类选最优 → 奖励模型更新</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th><strong>阶段</strong></th>
<th><strong>任务类型</strong></th>
<th><strong>目标&#x2F;能力</strong></th>
<th><strong>示例</strong></th>
</tr>
</thead>
<tbody><tr>
<td>预训练 (Pre-training)</td>
<td>图文对 (Image-Text Pair)</td>
<td>学习视觉语义对应</td>
<td>CLIP、BLIP-2 预训练阶段</td>
</tr>
<tr>
<td></td>
<td>语音-文本对</td>
<td>学习语音识别、跨模态对齐</td>
<td>Whisper、Speech2Text 模型</td>
</tr>
<tr>
<td></td>
<td>视频-文本对</td>
<td>学习事件描述与时间序列理解</td>
<td>Video Captioning、Video-Language Pretraining</td>
</tr>
<tr>
<td>指令微调 (Instruction Tuning,IT)</td>
<td>图像 QA</td>
<td>模型学会根据图片回答问题</td>
<td>LLaVA、InstructBLIP</td>
</tr>
<tr>
<td></td>
<td>图像描述生成</td>
<td>根据图片生成自然语言描述</td>
<td>Flamingo、BLIP-2 指令阶段</td>
</tr>
<tr>
<td></td>
<td>多模态对话</td>
<td>模型能在对话中理解图像内容</td>
<td>Chat with Image 模型、LLaVA 对话场景</td>
</tr>
<tr>
<td></td>
<td>跨模态推理</td>
<td>根据图像生成代码、解释现象等</td>
<td>Visual Code Generation、科学图表解释任务</td>
</tr>
</tbody></table>
<h3 id="MM-LLM的系统性分类"><a href="#MM-LLM的系统性分类" class="headerlink" title="MM-LLM的系统性分类"></a>MM-LLM的系统性分类</h3><p>多模态大模型的目标就是让不同模态的信息在某种统一空间下实现交互。<br>本质都是实现<strong>模态对齐</strong><br><em>模态对齐模式分类</em></p>
<div class="code-container" data-rel="Mathematica"><figure class="iseeu highlight mathematica"><table><tr><td class="code"><pre><span class="line">多模态对齐模式</span><br><span class="line">├── 显式模态对齐（<span class="variable">Explicit</span> <span class="built_in">Alignment</span>）</span><br><span class="line">│   ├── 预训练单模态模型 <span class="operator">+</span> 模态对齐</span><br><span class="line">│   ├── 模态适配器 <span class="punctuation">(</span><span class="variable">Adapter</span><span class="operator">-</span><span class="variable">based</span><span class="punctuation">)</span></span><br><span class="line">│   ├── 自监督跨模态学习（如 <span class="variable">CLIP</span>）</span><br><span class="line">│   └── 模态蒸馏 <span class="operator">/</span> 知识迁移（<span class="variable">Knowledge</span> <span class="variable">Distillation</span> <span class="operator">/</span> <span class="variable">Transfer</span>）</span><br><span class="line">│</span><br><span class="line">├── 隐式模态对齐（<span class="variable">Implicit</span> <span class="variable">Fusion</span>）<span class="operator">:</span>通过联合训练隐式学会对齐</span><br><span class="line">│   ├── 从头训练多模态模型</span><br><span class="line">│   └── 多阶段预训练（先模态内 → 再对齐 → 再任务）</span><br><span class="line">│</span><br><span class="line">├── 结构感知对齐（<span class="variable">Architecture</span><span class="operator">-</span><span class="variable">driven</span> <span class="operator">/</span> <span class="variable">Structural</span> <span class="built_in">Alignment</span>）<span class="operator">:</span> 强调语义单元而非整理表示</span><br><span class="line">│   ├── 模态 <span class="variable">MoE</span>（不同模态走不同 <span class="variable">expert</span> 分支，最后融合）</span><br><span class="line">│   └── 统一 <span class="variable">token</span> 化（<span class="variable">Modality</span> <span class="variable">Tokenization</span>，将模态离散为 <span class="variable">token</span>）</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>
<p><em>模态对齐层级分类</em><br> 维度: 对齐粒度(granularity)+对齐手段(mechanism)</p>
<table>
<thead>
<tr>
<th>对齐层级</th>
<th>位置</th>
<th>目标</th>
<th>主要方法</th>
<th>典型示例 &#x2F; 工程手段</th>
</tr>
</thead>
<tbody><tr>
<td><strong>表示层对齐 (Representation-level Alignment)</strong></td>
<td>输入模态经过各自 encoder → 进入 LLM 前</td>
<td>将不同模态输入映射到同一语义空间</td>
<td>- <strong>共享编码空间</strong>：多模态输入分别编码后，约束 embedding 相似（如 CLIP 图像-文本对齐）<br>- <strong>跨模态对比学习</strong>：最大化正样本相似度，最小化负样本相似度<br>- <strong>投影变换 &#x2F; Adapter</strong>：通过 projection head 或模态适配器映射到主模型 embedding 空间</td>
<td>模态适配器（Adapter）、CLIP、对比学习框架</td>
</tr>
<tr>
<td><strong>结构层对齐 (Structural Alignment)</strong></td>
<td>模态 encoder 输出的“结构化特征”对齐（时间 &#x2F; 空间 &#x2F; 图结构）</td>
<td>对齐模态之间的关系结构(token,patch,frame等)，而不仅是单点 embedding-&gt;对齐语义单位</td>
<td>- <strong>时序对齐</strong>：对齐语音&#x2F;视频片段与文本 token（如强制对齐、DTW）<br>- <strong>空间对齐</strong>：对齐图像区域与文本 token（region-text grounding、DETR-based）<br>- <strong>图结构对齐</strong>：建模为图，通过 Graph Matching 或 GNN 对齐</td>
<td>DTW、DETR-based 区域对齐、GNN</td>
</tr>
<tr>
<td><strong>语义层对齐 (Semantic Alignment)</strong></td>
<td>融合之后，送入 LLM 或任务头之前</td>
<td>保证不同模态表达的语义概念一致</td>
<td>- <strong>生成式对齐</strong>：一个模态生成另一个模态，再反向验证一致性（text→image→text）<br>- <strong>知识对齐</strong>：利用外部知识库确保语义统一（WordNet、ConceptNet）<br>- <strong>任务驱动对齐</strong>：下游任务监督信号强制学习可迁移语义</td>
<td>Text-to-Image-to-Text consistency、知识图谱辅助对齐、下游任务监督</td>
</tr>
<tr>
<td><strong>优化层对齐 (Optimization-level Alignment)</strong></td>
<td>训练范式层面（影响整个 pipeline）</td>
<td>从训练&#x2F;优化角度保证不同模态学习的平衡与协同</td>
<td>- <strong>联合训练 + 多任务权重平衡</strong>（GradNorm、Dynamic Weight Averaging）<br>- <strong>互信息最大化</strong>（Deep InfoMax 风格）<br>- <strong>蒸馏式对齐</strong>：teacher-student 模态对齐</td>
<td>GradNorm、多任务训练、互信息最大化、模态蒸馏</td>
</tr>
<tr>
<td><strong>任务层对齐（Task-level Alignment）</strong></td>
<td>下游任务输出阶段</td>
<td>确保多模态输出在任务语义上一致（如 VQA 中图像推理与文本回答逻辑一致）</td>
<td>一致性损失、多任务联合训练、强化学习反馈</td>
<td>Video-LLaMA 中视频理解与问答一致性约束</td>
</tr>
</tbody></table>
<p><em>融合策略分类</em></p>
<table>
<thead>
<tr>
<th>融合方式</th>
<th>类别</th>
<th>思路</th>
<th>优点</th>
<th>缺点</th>
<th>典型例子</th>
</tr>
</thead>
<tbody><tr>
<td>**Early Fusion **</td>
<td>输入级融合</td>
<td>将多模态输入拼接后直接输入 Transformer</td>
<td>简单、直接</td>
<td>模态差异大时效果差</td>
<td>一些早期 VLP 模型</td>
</tr>
<tr>
<td>**Late Fusion **</td>
<td>输出级融合</td>
<td>模态各自独立编码，最后在高层融合结果</td>
<td>保留各模态独立性</td>
<td>缺乏深层交互</td>
<td>部分多模态检索模型</td>
</tr>
<tr>
<td><strong>Cross-Attention Fusion</strong></td>
<td>表示&#x2F;结构对齐后</td>
<td>一个模态作为 query，另一个模态作为 key&#x2F;value 做 cross-attention</td>
<td>能学到强交互</td>
<td>计算量大</td>
<td>BLIP-2 (Q-Former)、LLaVA</td>
</tr>
<tr>
<td><strong>Projection + Frozen LLM</strong></td>
<td>表示层对齐阶段</td>
<td>只训练轻量 projection（Adapter），将模态特征投影到 LLM embedding 空间，LLM 权重冻结</td>
<td>参数高效</td>
<td>对齐能力受限，语义对齐需大数据</td>
<td>LLaVA、MiniGPT-4、BLIP-2</td>
</tr>
<tr>
<td><strong>Unified Transformer</strong></td>
<td>全模态统一建模</td>
<td>所有模态都 token 化，统一输入到一个 Transformer</td>
<td>模态对称，语义一致</td>
<td>训练成本极高</td>
<td>Flamingo(<strong>统一输入空间</strong>:图像patch和文本token进行拼接)、Kosmos-1(统一语义空间但分离编码: 各自编码+注意力交互)、PaLM-E</td>
</tr>
<tr>
<td><strong>Mixture-of-Experts (MoE for Modalities)</strong></td>
<td>融合阶段 &#x2F; LLM 内部</td>
<td>不同模态通过 MoE 路由到不同专家，再在 LLM 中融合</td>
<td>可扩展新模态</td>
<td>调度复杂，优化难</td>
<td>DeepMind Gemini、部分 Google 最新工作</td>
</tr>
</tbody></table>
<p><em>训练模式分类</em></p>
<table>
<thead>
<tr>
<th><strong>训练模式</strong></th>
<th><strong>核心思路</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
<th><strong>典型例子</strong></th>
<th><strong>适用场景&#x2F;备注</strong></th>
<th>解释</th>
<th>实现方式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>预训练单模态模型 + 模态对齐</strong></td>
<td>利用已有单模态 backbone（如 CLIP 图像 encoder、GPT 文本 encoder），在联合表示空间对齐</td>
<td>计算效率高，充分利用已有模型知识</td>
<td>受限于原单模态模型的表示能力</td>
<td>BLIP-2, LLaVA</td>
<td>当已有强单模态模型时最优；适合快速原型</td>
<td>模态对齐: 把不同模态的特征放到一个<strong>共享表示空间</strong>中，让模型能“对齐”图像 ↔ 文本 ↔ 音频等</td>
<td>对比学习: CLIP: 让图像特征和文本特征尽量接近<br> 多模态Transformer: 同时输入不同模态特征，直接融合</td>
</tr>
<tr>
<td><strong>从头训练多模态模型</strong></td>
<td>直接训练 joint model，把不同模态一起学习</td>
<td>各模态表征天然统一，统一性强</td>
<td>训练代价大，数据需求高</td>
<td>Flamingo, PaLM-E</td>
<td>大规模数据和算力充足，追求最优联合表征</td>
<td>模型从零开始学习跨模态关联，无需依赖预训练单模态模型</td>
<td>联合输入多模态数据，通过统一架构（如 Transformer）进行联合训练</td>
</tr>
<tr>
<td><strong>模态适配器 &#x2F; 轻量调优 (Adapter-based)</strong></td>
<td>保持 LLM 不变，在输入端加小 adapter，把视觉&#x2F;音频&#x2F;信号映射到 embedding 空间</td>
<td>参数高效，不需大规模重训</td>
<td>Adapter 容量有限，难完全融合模态知识</td>
<td>LLaVA (linear projection), Q-Former (BLIP-2)</td>
<td>模型复用性强，小规模增量任务</td>
<td>模态适配器:在保持大语言模型结构不变的情况下，把非文本模态映射到 LLM 的输入 embedding 空间</td>
<td>- 简单线性投影 (LLaVA)。<br>    <br>- Q-Former（BLIP-2 里用小 Transformer 作为 adapter）</td>
</tr>
<tr>
<td><strong>多阶段预训练 (Stage-wise Pretraining)</strong></td>
<td>先做模态内预训练，再做模态间对齐，最后任务微调</td>
<td>逐步迁移，训练稳定</td>
<td>流程复杂</td>
<td>ALIGN, ALBEF</td>
<td>数据充足但算力有限时，保证训练稳定性</td>
<td>逐步构建跨模态理解能力，避免端到端训练的不稳定性</td>
<td>阶段1：图像&#x2F;文本分别预训练；阶段2：对比学习对齐；阶段3：下游任务微调</td>
</tr>
<tr>
<td><strong>模态蒸馏 &#x2F; 知识迁移</strong></td>
<td>利用强单模态教师模型，把知识蒸馏到多模态学生模型</td>
<td>学生模型小，推理快</td>
<td>学生性能受教师上限约束</td>
<td>Distilled CLIP, MiniGPT-4</td>
<td>想做轻量部署或推理高效模型</td>
<td>通过软标签或特征对齐，将教师模型的跨模态知识迁移到学生模型</td>
<td>特征蒸馏、logits 蒸馏、注意力迁移等</td>
</tr>
<tr>
<td><strong>模态 MoE (Mixture of Experts)</strong></td>
<td>不同模态输入走不同 expert 分支，最后融合</td>
<td>可扩展，每次只激活部分参数，节省计算</td>
<td>路由策略复杂，训练不稳定</td>
<td>GLaM-like 多模态扩展, Perceiver IO</td>
<td>面向大规模、多模态扩展任务</td>
<td>利用稀疏激活机制实现高效多模态建模</td>
<td>基于输入模态或内容动态选择 expert，加权融合结果</td>
</tr>
<tr>
<td><strong>统一 token 化 (Modality Tokenization)</strong></td>
<td>所有模态离散化为类似文本 token，然后直接用 LLM 处理</td>
<td>极端统一 → LLM 可直接处理多模态</td>
<td>信息损失大，对 tokenizer 依赖强</td>
<td>VQ-VAE + GPT, BEiT, MusicLM</td>
<td>模态高度异质但想用统一 LLM 架构</td>
<td>将非文本信号“翻译”成语言模型可理解的符号序列</td>
<td>使用 VQ-VAE、dVAE 等将图像&#x2F;音频编码为离散 token</td>
</tr>
<tr>
<td><strong>自监督跨模态学习</strong></td>
<td>利用海量未标注数据，通过对比&#x2F;掩码&#x2F;自回归预训练</td>
<td>不依赖标注，扩展性强</td>
<td>难以控制对齐质量</td>
<td>CLIP, BEiT-3</td>
<td>海量数据下对齐模态关系，适合预训练阶段</td>
<td>通过数据内在结构（如图文共现）学习语义对齐</td>
<td>• 对比学习（CLIP）  <br>• 掩码多模态建模（BEiT-3）  <br>• 跨模态自回归预测</td>
</tr>
</tbody></table>
<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ul>
<li>是否可以删除MMLLM的一些模块，简化假设？ 可以但是算力会增加很多(比如:所有模态都看作离散的token)</li>
<li>是否有信号为Backbone+文本为接口 的大模型？在信号处理领域，有把音频&#x2F;IQ信号&#x2F;时间序列作为核心的思路，但还不是主流。缺少统一的表达与交互语言。</li>
</ul>
<h2 id="ViT-Vision-Transformer"><a href="#ViT-Vision-Transformer" class="headerlink" title="ViT(Vision Transformer)"></a>ViT(Vision Transformer)</h2><table>
<thead>
<tr>
<th>项目</th>
<th>内容</th>
</tr>
</thead>
<tbody><tr>
<td><strong>目的</strong></td>
<td>用 Transformer 机制替代 CNN 提升全局感知能力</td>
</tr>
<tr>
<td><strong>架构</strong></td>
<td>Patch Embedding → Transformer Encoder → MLP Head</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>全局特征建模强、可扩展性好、迁移性优、解释性强</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>数据需求大、计算复杂、缺乏局部先验</td>
</tr>
<tr>
<td><strong>效果</strong></td>
<td>在大规模数据集上性能超越 CNN，成为视觉模型新主流</td>
</tr>
<tr>
<td>![[Pasted image 20251004182411.png]]</td>
<td></td>
</tr>
<tr>
<td>注: 如果 <strong>patch 切得过大</strong>，确实会 <strong>丢失局部的二维位置信息</strong></td>
<td></td>
</tr>
</tbody></table>
<h2 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h2><p>核心思想: <strong>拉近正样本对的表示距离，推远负样本对的表示距离</strong>，从而在嵌入空间中构造有意义的语义结构</p>
<h3 id="CLIP-Contrastive-Language-Image-Pretraining"><a href="#CLIP-Contrastive-Language-Image-Pretraining" class="headerlink" title="CLIP(Contrastive Language-Image Pretraining)"></a>CLIP(Contrastive Language-Image Pretraining)</h3><p>目标: 把图像和文本映射到同一个语义空间<br>机制: 对比学习</p>
<h3 id="常见应用"><a href="#常见应用" class="headerlink" title="常见应用"></a>常见应用</h3><table>
<thead>
<tr>
<th align="center"><strong>领域</strong></th>
<th align="left"><strong>代表方法</strong></th>
<th align="left"><strong>正样本定义</strong></th>
<th align="left"><strong>负样本来源</strong></th>
<th align="left"><strong>应用场景</strong></th>
<th align="left"><strong>主要创新与特点</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>计算机视觉 (CV)</strong></td>
<td align="left"><strong>SimCLR (2020, Google)</strong></td>
<td align="left">同一张图像的两次随机增强视图</td>
<td align="left">同一 batch 内其他图像</td>
<td align="left">无监督图像表征学习</td>
<td align="left">纯对比学习，无需标签；使用 InfoNCE 损失，结构简洁但性能强</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>MoCo (2020, Facebook)</strong></td>
<td align="left">同一图像的不同增强视图</td>
<td align="left">动量更新的负样本队列</td>
<td align="left">大规模图像表征、检测、分类</td>
<td align="left">引入动量编码器与队列机制，维持动态负样本库，提升训练稳定性</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>BYOL (2020, DeepMind)</strong></td>
<td align="left">同一图像的两种增强视图（Teacher–Student）</td>
<td align="left">无显式负样本</td>
<td align="left">自监督表征学习</td>
<td align="left">通过动量教师与停止梯度避免坍塌，去除对负样本依赖</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>SimSiam (2021, Meta)</strong></td>
<td align="left">同一图像的不同增强视图</td>
<td align="left">无显式负样本</td>
<td align="left">轻量级表征学习</td>
<td align="left">去掉动量教师，仅靠停止梯度维持稳定；结构更简洁</td>
</tr>
<tr>
<td align="center"><strong>自然语言处理 (NLP)</strong></td>
<td align="left"><strong>SimCSE (2021)</strong></td>
<td align="left">同一句子经不同 dropout 得到的两种结果</td>
<td align="left">Batch 内其他句子</td>
<td align="left">句子 embedding、语义检索</td>
<td align="left">对比学习提升句子表示语义一致性；方法极简高效</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>InfoXLM &#x2F; LaBSE</strong></td>
<td align="left">平行语料中的中英文句子</td>
<td align="left">其他句子</td>
<td align="left">跨语言语义对齐</td>
<td align="left">学到语言无关语义空间，用于跨语言检索与翻译</td>
</tr>
<tr>
<td align="center"><strong>语音 &#x2F; 音频 (Speech &amp; Audio)</strong></td>
<td align="left"><strong>Wav2Vec 2.0 (2020, Facebook)</strong></td>
<td align="left">被 mask 的语音片段 vs 预测 embedding</td>
<td align="left">负样本来自非目标帧</td>
<td align="left">语音识别、音频表征</td>
<td align="left">Mask + 对比预测；在低标注数据下表现优异</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>HuBERT (2021)</strong></td>
<td align="left">相同语音的不同 masked 区域预测</td>
<td align="left">其他 cluster 中样本</td>
<td align="left">语音表征、自监督 ASR</td>
<td align="left">结合聚类与预测任务，形成自监督标签；提升表征质量</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>CLAP (2022)</strong></td>
<td align="left">音频与对应文本描述</td>
<td align="left">其他音频或文本</td>
<td align="left">音频–文本跨模态检索</td>
<td align="left">类似 CLIP 的音频版本；实现听觉语义对齐与跨模态理解</td>
</tr>
<tr>
<td align="center"><strong>跨模态 (Multi-Modal)</strong></td>
<td align="left"><strong>CLIP (2021, OpenAI)</strong></td>
<td align="left">图像与对应文本说明</td>
<td align="left">其他图文对</td>
<td align="left">零样本分类、图文检索</td>
<td align="left">首次大规模图文对比预训练；双塔结构；强泛化与零样本能力</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>ALIGN (2021, Google)</strong></td>
<td align="left">图像与对应文本</td>
<td align="left">大规模网页数据中的负样本</td>
<td align="left">多模态检索、语义对齐</td>
<td align="left">使用上亿图文对，弱标注数据；高效大规模预训练</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>LiT (2022, Google)</strong></td>
<td align="left">冻结文本模型 + 匹配图像</td>
<td align="left">其他样本</td>
<td align="left">高效多模态对齐</td>
<td align="left">避免联合训练，仅调图像模型；快速迁移已有语言模型</td>
</tr>
<tr>
<td align="center"></td>
<td align="left"><strong>Audio–Video CL</strong></td>
<td align="left">视频帧与对应音频片段</td>
<td align="left">其他片段</td>
<td align="left">视频理解、跨模态检索</td>
<td align="left">捕捉声–视同步关系，学习时空跨模态表示</td>
</tr>
</tbody></table>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><table>
<thead>
<tr>
<th><strong>类别</strong></th>
<th><strong>核心思路</strong></th>
<th><strong>代表算法</strong></th>
<th><strong>典型应用场景</strong></th>
<th>优缺点简述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>基于价值的方法 (Value-based RL)</strong></td>
<td>通过学习 <strong>状态-动作价值函数 Q(s,a)</strong> 来指导决策，选择期望回报最高的动作</td>
<td>Q-Learning、Deep Q-Network (DQN)</td>
<td>Atari 游戏（打乒乓、吃豆人等）</td>
<td>✅ 稳定高效，适合离散动作；❌ 难处理连续控制</td>
</tr>
<tr>
<td><strong>基于策略的方法 (Policy-based RL)</strong></td>
<td>直接学习策略 π(as)，通过优化期望回报来更新策略参数</td>
<td>REINFORCE</td>
<td></td>
<td>连续动作控制（机器人走路）</td>
</tr>
<tr>
<td><strong>Actor-Critic 方法（结合价值+策略）</strong></td>
<td>同时学习策略（Actor）和价值函数（Critic），相互指导提升稳定性</td>
<td>A3C、PPO、DDPG</td>
<td>自动驾驶决策、复杂运动控制</td>
<td>✅ 稳定性好；❌ 超参数多、调优复杂</td>
</tr>
<tr>
<td><strong>基于模型的方法 (Model-based RL)</strong></td>
<td>学习环境的动态模型，用模型进行 <strong>内部模拟（planning）</strong> 来提升效率</td>
<td>Dyna-Q、AlphaZero</td>
<td>围棋、国际象棋、规划控制</td>
<td>✅ 样本效率高；❌ 模型误差敏感</td>
</tr>
<tr>
<td><strong>层次化强化学习 (Hierarchical RL)</strong></td>
<td>将任务分解为多个子任务（高层规划、低层执行）</td>
<td>Options Framework、HRL</td>
<td>机器人执行多步复杂任务（如“走到桌子→抓起杯子”）</td>
<td>✅ 可扩展复杂任务；❌ 层次设计困难</td>
</tr>
<tr>
<td><strong>多智能体强化学习 (Multi-Agent RL)</strong></td>
<td>多个智能体同时学习与博弈，通过协作或竞争优化整体策略</td>
<td>MADDPG、QMIX</td>
<td>多无人机协作、群体智能、博弈系统</td>
<td>✅ 支持协作&#x2F;竞争；❌ 收敛困难、通信复杂</td>
</tr>
<tr>
<td><strong>基于人类反馈的强化学习 (RLHF)</strong></td>
<td>用人类偏好信号代替环境奖励，训练奖励模型指导策略优化</td>
<td>PPO（with reward model）、DPO</td>
<td>LLM &#x2F; MMLLM 的人类价值对齐（如 ChatGPT、Gemini、Qwen-VL）</td>
<td>✅ 能融入人类价值观；❌ 标注成本高、奖励偏差风险</td>
</tr>
</tbody></table>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 林夕. 《# 多模态大模型入门指南-长文慎入【持续更新】》<a class="link"   href="https://zhuanlan.zhihu.com/p/682893729" >https://zhuanlan.zhihu.com/p/682893729<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><br>[2].OpenAI-GPT-5<br>[3].Qwen3-Max<br>[4].Deepseek-V3.2</p>
]]></content>
      <tags>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读2</title>
    <url>/2025/09/27/template/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB2/</url>
    <content><![CDATA[<h1 id="📑"><a href="#📑" class="headerlink" title="📑 "></a>📑 论文阅读2</h1><blockquote>
<p>作者, 会议&#x2F;期刊, 年份</p>
</blockquote>
<hr>
<h2 id="1-研究问题"><a href="#1-研究问题" class="headerlink" title="1. 研究问题"></a>1. 研究问题</h2><ul>
<li>论文要解决的核心问题是什么？</li>
<li>这个问题在 IoT&#x2F;AI 或更大背景下为什么重要？</li>
</ul>
<hr>
<h2 id="2-第一性原理层（上限）"><a href="#2-第一性原理层（上限）" class="headerlink" title="2. 第一性原理层（上限）"></a>2. 第一性原理层（上限）</h2><ul>
<li>本问题的物理&#x2F;信息论&#x2F;数学“硬约束”是什么？</li>
<li>作者是否明确提到这些上限？</li>
<li>我的理解：＿＿＿＿</li>
</ul>
<hr>
<h2 id="3-归纳假设层（条件）"><a href="#3-归纳假设层（条件）" class="headerlink" title="3. 归纳假设层（条件）"></a>3. 归纳假设层（条件）</h2><ul>
<li>作者假设了什么环境&#x2F;条件？<ul>
<li>数据分布？</li>
<li>系统模型？</li>
<li>攻击者&#x2F;用户能力？</li>
</ul>
</li>
<li>这些假设现实吗？如果放松假设会怎样？</li>
<li>我的判断：＿＿＿＿</li>
</ul>
<hr>
<h2 id="4-方法实现层（模型-系统）"><a href="#4-方法实现层（模型-系统）" class="headerlink" title="4. 方法实现层（模型&#x2F;系统）"></a>4. 方法实现层（模型&#x2F;系统）</h2><ul>
<li>作者的方法核心思路：<ul>
<li>算法框架：</li>
<li>系统架构：</li>
<li>实现细节：</li>
</ul>
</li>
<li>这是基于第一性原理，还是基于某些归纳假设的技巧？</li>
</ul>
<hr>
<h2 id="5-结果与评价"><a href="#5-结果与评价" class="headerlink" title="5. 结果与评价"></a>5. 结果与评价</h2><ul>
<li>实验设计（数据集&#x2F;设备&#x2F;仿真环境）：</li>
<li>使用的指标：</li>
<li>对比基线：</li>
<li>主要结果：</li>
<li>我的怀疑&#x2F;思考：＿＿＿＿</li>
</ul>
<hr>
<h2 id="6-创新与局限"><a href="#6-创新与局限" class="headerlink" title="6. 创新与局限"></a>6. 创新与局限</h2><ul>
<li>创新点（与已有方法相比的新思路）：</li>
<li>局限性（依赖的假设、无法覆盖的场景、实验不足之处）：</li>
</ul>
<hr>
<h2 id="7-我的迁移思考"><a href="#7-我的迁移思考" class="headerlink" title="7. 我的迁移思考"></a>7. 我的迁移思考</h2><ul>
<li>能否迁移到 IoT+AI 的其他问题？</li>
<li>能否跨领域借鉴（类比到图像、信号、边缘计算等）？</li>
<li>潜在改进方向（可以做的 follow-up 实验&#x2F;论文点子）：</li>
</ul>
<hr>
<h2 id="8-总结（1–2-句话）"><a href="#8-总结（1–2-句话）" class="headerlink" title="8. 总结（1–2 句话）"></a>8. 总结（1–2 句话）</h2><ul>
<li>我从这篇论文学到的最重要的一点：</li>
<li>这篇论文在我的研究里的意义：</li>
</ul>
<hr>
<p>请你用“第一性原理 + 系统化分析框架”帮我分析论文《{论文题目}》。</p>
<p>分析结构如下：</p>
<ol>
<li><p><strong>问题背景与第一性原理</strong>  </p>
<ul>
<li>研究对象的本质约束是什么？（算力、资源、信息结构…）  </li>
<li>作者想解决的第一性问题是什么？</li>
</ul>
</li>
<li><p><strong>研究假设与方法</strong>  </p>
<ul>
<li>作者隐含的归纳假设是什么？  </li>
<li>使用了什么方法&#x2F;架构来支撑这些假设？</li>
</ul>
</li>
<li><p><strong>实验与验证</strong>  </p>
<ul>
<li>实验设计思路？  </li>
<li>评估指标与对比对象？  </li>
<li>关键结果（从本质角度解读，而不是细节罗列）。</li>
</ul>
</li>
<li><p><strong>局限性</strong>  </p>
<ul>
<li>哪些约束没有被解决？  </li>
<li>在现实部署中可能遇到哪些困难？</li>
</ul>
</li>
<li><p><strong>未来方向</strong>  </p>
<ul>
<li>如何进一步突破上限？  </li>
<li>跨领域的可能延伸？</li>
</ul>
</li>
<li><p><strong>总结</strong>  </p>
<ul>
<li>论文贡献在“第一性原理”的语境下如何定位？  </li>
<li>对 IoT + AI 交叉研究的启发是什么？</li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>AIoT</tag>
        <tag>Paper</tag>
        <tag>Notes</tag>
      </tags>
  </entry>
</search>
